%Preamble
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{setspace}
\setstretch{1.5}
\usepackage{geometry}
\geometry{a4paper, portrait, margin=30mm, bmargin=30mm, tmargin=30mm}
\usepackage{amsmath}
\usepackage{esvect}

\newcommand{\pvec}[1]{\vec{#1}\mkern2mu\vphantom{#1}}

%Bibliography
%\usepackage[backend = biber, style = alphabetic, sorting = ynt]{biblatex}
%\addbibresource{thesis.bib}
\bibliographystyle{unsrt}

%Patch abstract command to make it normal size
\makeatletter
\renewenvironment{abstract}{%
    \if@twocolumn
      \section*{\abstractname}%
    \else %% <- here I've removed \small
      \begin{center}%
        {\bfseries \Large\abstractname\vspace{\z@}}%  %% <- here I've added \Large
      \end{center}%
      \quotation
    \fi}
    {\if@twocolumn\else\endquotation\fi}
\makeatother


\begin{document}
\begin{center}
\thispagestyle{empty}
\large{\textbf{Univserity of Innsbruck}}\\[-0.9ex]
\large{Department of General, Inorganic and Theoretical Chemistry}\\
\vspace{0.3cm}
\begin{center}
\includegraphics[width=7cm]{Images/Logo.jpg}\\
\vspace{0.9cm}
\textbf{\LARGE{Master Thesis}}
\medskip\par
\vspace{1.2cm}
\Large{\textbf{Structure and Thermodynamics of \\ Guest@MOF Material}}\\[-0.5ex]
\vspace*{1.5cm}
\large{Investigating GUEST in MIL-68Ga via a Molecular Dynamics Simulation Approach}\\[-1.5ex]
\vspace*{1.5cm}
\bigskip\par
\textbf{Michael Helmut Fill, BSc. }\\[-1ex]
\medskip
\textbf{Supervisor:} Assoc. Prof. Dr. Thomas Hofer\\
Hier Datum einfügen
\end{center}
\end{center}

\newpage

\begin{abstract}
  This thesis is dedicated to the brave Mujahideen fighters of Afghanistan.
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}

\section{Theory}
\subsection{Quantum Chemistry}
At the dawn of the 20th century, general consensus among many physicists versed in classical mechanics was that the
fundamental laws of nature had been solved, and that the by then still unsolved problems of e.g.~black body radiation
would be resolved in due time. However, when Max Planck published his ideas on the quantization of energy in 1900~\cite{Planck1901}, and especially
after Albert Einstein realistically explained the photoelectric effect using Plancks' hypothesis in 1905~\cite{Einstein1905}, it became clear that the classical laws of physics were 
inadequate at explaining the behavior of matter on the atomic scale.
The field of quantum mechanics was born, and together with further groundbreaking contributions by, among others, Niels Bohr~\cite{Bohr1913}, Werner Heisenberg~\cite{Heisenberg1927} and Louis de Broglie~\cite{Broglie1924},
paved the way for Erwin Schrödinger to lay the groundwork for quantum chemistry with his wave equation in 1926~\cite{Schrdinger1926}\cite{Schrdinger1926-2}.
\subsubsection{The Schrödinger Equation}
Starting from the initial assumption that all properties of a given system could be described by a wave function $\Psi$, Schrödinger derived his now well-known
time-independant~(\ref{eq:schrodingerIndependant}) and, more general, time-dependant~(\ref{eq:schrodingerDependant}) equations.


\begin{equation}
  \hat{H}\Psi = E\Psi
  \label{eq:schrodingerIndependant}
\end{equation}
\begin{equation}
  i\hbar\frac{\partial}{\partial t}\Psi = \hat{H}\Psi
  \label{eq:schrodingerDependant}
\end{equation}

\bigskip

\noindent With $\Psi$ being the eigenfunction of the system, $\hbar$ the reduced Planck constant, $\hat{H}$ the Hamiltonian operator, $t$ the time and $E$ the energy of the system, or eigenvalue of the Hamiltonian operator.
The Hamiltonian operator~(\ref{eq:hamiltonian}) is defined as the sum of the kinetic and potential energy operators, which, when applied to a state function $\Psi$, yields the total energy of the system as its eigenvalue to the eigenfunction $\Psi$.

\begin{equation}
  \hat{H} = -\frac{\hbar^2}{2m}\nabla^2 + E_{pot} = \hat{T} + \hat{V}
  \label{eq:hamiltonian}
\end{equation}

\bigskip

\noindent Here, $\nabla^2$ is the Laplace operator, $m$ the mass of the particle and $E_{pot}$ the potential energy of the system, with the kinetic energy and potential energy operators historically referred to as $\hat{T}$ and $\hat{V}$ respectively.
For a single particle with mass $m$ moving in three-dimensional space, the time independant Schrödinger equation can then be written as~(\ref{eq:differential}).

\begin{equation}
  -\frac{\hbar^2}{2m}\nabla^2\Psi + E_{pot}\Psi = E\Psi
  \label{eq:differential}
\end{equation}

\begin{equation*}
  \nabla^2 = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}
\end{equation*}

\bigskip

\noindent This linear differential equation~(\ref{eq:differential}) forms the basis for determining the wave function of a given system. 
A wavefunction $\Psi$ that satisfies the Schrödinger equation and results in its respective eigenvalue $E$ must therefore be an eigenfunction to the Hamiltonian operator.
It is due to this that finding the correct wave function $\Psi$ for a system is referred to as an eigenvalue problem.\\
Hereby, it is important to note that a wavefunction is not directly observable and therefore no meaning can be attributed to its values.
However, the square modulus of the wavefunction $|\Psi|^2$, as interpreted by Max Born~\cite{Born1926}, is proportional to the observable probability density of an electron in the given system.

\subsubsection{Hamiltonian Operator}
As previously mentioned, the Hamiltonian operator is defined as the sum of kinetic and potential energies in a given system. 
The notation given in equation~\ref{eq:hamiltonian} is a simplification of the actual Hamiltonian operator, which can be written as in equation~\ref{eq:hamiltonian3} and~\ref{eq:hamiltonian2} as them sum of its individual terms, being the kinetic energy of the nuclei $\hat{T}_N$, the kinetic energy of the electrons $\hat{T}_e$, the potential energy of the nuclei $\hat{V}_{NN}$, the potential energy of the nuclei and electrons $\hat{V}_{Ne}$ and the potential energy of the electrons $\hat{V}_{ee}$.

\begin{equation}
  \hat{H} = \hat{T}_N + \hat{T}_e + \hat{V}_{NN} + \hat{V}_{Ne} + \hat{V}_{ee}
  \label{eq:hamiltonian3}
\end{equation}

\begin{equation}
  \begin{aligned}
    \hat{H} = & -\sum_{I=1}^{N}\frac{\hbar^2}{2m_I}\nabla_I^2 -\sum_{i=1}^{n}\frac{\hbar^2}{2m_e}\nabla_i^2 + \frac{1}{2} \sum_{I\neq J}^{N} \frac{q_{e}^2}{4\pi \epsilon_0}\frac{Z_I Z_J}{|r_I - r_J|} \\
    & - \sum_{I=1}^{N}\sum_{i=1}^{n}\frac{q_{e}^2}{4\pi \epsilon_0}\frac{Z_I}{|r_I - r_i|} + \frac{1}{2}\sum_{i\neq j}^{n}\frac{q_{e}^2}{4\pi \epsilon_0}\frac{1}{|r_i - r_j|}
  \end{aligned}
  \label{eq:hamiltonian2}
\end{equation}

\bigskip

\noindent With $N$ being the number of nuclei, $n$ the number of electrons, $m_I$ the mass of the $I$-th nucleus, $m_e$ the mass of the electron, $Z_I$ the charge of the $I$-th nucleus, $r_I$ the position of the $I$-th nucleus, $r_i$ the position of the $i$-th electron, $q_e$ the elementary charge and $\epsilon_0$ the vacuum permittivity.
To simplify this expression, atomic units are introduced, leading to the Hamiltonian as shown in equation~\ref{eq:hamiltonianAtomic}.

\bigskip

\begin{equation}
  \begin{aligned}
    \hat{H} = & -\sum_{I=1}^{N}\frac{1}{2m_I}\nabla_I^2 -\sum_{i=1}^{n}\frac{1}{2}\nabla_i^2 + \frac{1}{2} \sum_{I\neq J}^{N} \frac{Z_I Z_J}{|r_I - r_J|} \\
    & - \sum_{I=1}^{N}\sum_{i=1}^{n}\frac{Z_I}{|r_I - r_i|} + \frac{1}{2}\sum_{i\neq j}^{n}\frac{1}{|r_i - r_j|}
  \end{aligned}
  \label{eq:hamiltonianAtomic}
\end{equation}

\bigskip

\subsubsection{Dirac Notation}
Since it is not feasible and often impossible to solve the Schrödinger equation for systems of 3 or more unconstrained particles, should a coulombic potential be employed~\cite{Toli2019}, a number of approximations are made, some of which will be discussed in the following sections. 
To work with these approximated wave functions, the expressions for the Schrödinger equation must be adjusted as in~\ref{eq:dirac}. 

\begin{equation}
  \hat{H}\Psi = E\Psi
  \label{eq:dirac}
\end{equation}
\begin{equation*}
  \Psi^{*}\hat{H}\Psi = \Psi^{*}E\Psi
\end{equation*}
\begin{equation*}
  \int_{-\infty}^{\infty}\Psi^{*}\hat{H}\Psi d\tau = \int_{-\infty}^{\infty}\Psi^{*}E\Psi d\tau
\end{equation*}

\bigskip

\noindent With $\tau$ being the volume element of the system and $\Psi^{*}$ the complex conjugate of the wave function $\Psi$.
The received energy, granted that $\Psi$ is an approximated wave function, is now an estimate of the actual analytical energy.
To simplify these integral expressions the \textit{bra-ket} notation, also known as \textit{Dirac} notation~\cite{Dirac1939}, is employed as shown in equation~\ref{eq:dirac2}.

\begin{equation}
  \langle\Psi|\hat{H}|\Psi\rangle = \langle\Psi|E|\Psi\rangle
  \label{eq:dirac2}
\end{equation}

\bigskip

\subsubsection{Variational Principle}
With the framework for utilizing approximated wave functions in place, the next step is to qualify a chosen wave function according to its accurary as an approximation to the analytical wave function.
Since the energy of a system is constant, it can be taken out of the integral expression, leaving the remaining normalized integral yielding 1.
Choosing now a trial wave function $\Psi_{Trial}$ and applying the Hamiltonian operator to it, the resulting energy expectation value is always greater or equal to the ground state energy of the system, as shown in equation~\ref{eq:variational}.
This method of finding parametres to construct the best possible approximated wave function is called the variational method and sits at the heart of many quantum mechanical approaches~\cite{Griffiths2017}.

\begin{equation}
  \langle\Psi_{Trial}|\hat{H}|\Psi_{Trial}\rangle = \langle E\rangle \geq E_0
  \label{eq:variational}
\end{equation}

\bigskip

\subsection{Hartree-Fock Theory}
As previously mentioned, the exact solution to the Schrödinger equation for systems containing more than 2 unconstrained particles is analytically not feasible.
To this end, several approximations are employed to simplify the eigenvalue problem down to an analytically solvable form.
The Hartree-Fock method employs such approximations and provides a fundamental framework for the analysis of many-electron systems.

\subsubsection{Non-relativistic stationary systems}
An important assumption not exclusive to Hartree-Fock Theory is the non-relativistic nature of the analysed system, 
implying stationary systems with velocities much smaller than the speed of light.
This assumption allows one to employ only the much simpler time-independant Schrödinger equation, with the trade-off being that effects arising from special relativity, e.g.~spatial contraction or time dilation in heavy atoms, are not accounted for.
Additionally, since only stationary states are computed, the analysis of time-dependant processes require specific simulation frameworks, one of which will be discussed at a later point.

\subsubsection{Born-Oppenheimer Approximation}
The Hartree-Fock method inherently assumes the Born-Oppenheimer approximation, which states that the wave functions of nuclei and electrons of a given system may be decoupled~\cite{Born1927}.
This seperation is justified via the large mass difference of a factor of 1836 between electrons and protons, and allows for the assumption that nuclei may be considered stationary during the calculation of the electronic wave functions.

\begin{equation}
  |\Psi^{n, N}\rangle = |\Psi^{n}\rangle|\Psi^{N}\rangle 
\end{equation}

\bigskip

\noindent With $n$ and $N$ denoting the electronic and nuclear wave functions respectively. As a result of this approximation,
the term for the kinetic energy of the nuclei in the Hamiltonian is omitted, while the coulombic repulsion between different nuclei becomes a constant term, further simplifying calculations.
An eigenfunction to this electric Hamiltonian, as shown in~\ref{eq:electricHamiltonian},  is now referred to as an electronic eigenfunction to the Schrödinger equation.

\begin{equation}
  \hat{H} = -\sum_{i=1}^{n}\frac{1}{2}\nabla_i^2 -\sum_{I=1}^{N}\sum_{i=1}^{n}\frac{Z_I}{|r_I - r_i|} + \frac{1}{2}\sum_{i\neq j}^{n}\frac{1}{|r_i - r_j|}  \label{eq:electricHamiltonian}
\end{equation}

\bigskip

\noindent Neglecting the quantum character of nuclei in this way renders the electronic Hamiltonian useless for describing quantum mechanical phenomena involving nuclei, yet allows for vastly increased computational efficiency.

\subsubsection{Independant Particle approximation}
Douglas R. Hartree provided a further approximation to n-electron wave functions in 1928 in the form of a product of independant one-electron functions, referred to as \textit{Hartree product}~\cite{Hartree1928}.
Equation~\ref{eq:hartreeProduct} shows this formalism, where each independant electrons $i$ interaction with all other $n-1$ electrons of the system is averaged out, referred to as a \textit{mean field approximation}.

\begin{equation}
  |\Psi^{n}\rangle = \prod_{i=1}^{n}|\psi_i\rangle
  \label{eq:hartreeProduct}
\end{equation}

\subsubsection{Slater Determinant}
While providing a good approximation to n-electron wavefunctions, the Hartree product does not account for the antisymmetrical nature of fermions due to their half-integer spin resulting from the Pauli exclusion principle~\cite{Pauli1925}.
In essence, this means that transposing any two, by definition indistinguishable, electrons in a wave function must result in a sign change. For a two particle system with coordinates $r_1$ and $r_2$, this may be written as seen in~\ref{eq:antisymmetry}.

\begin{equation}
  \Psi(r_1, r_2) = -\Psi(r_2, r_1)
  \label{eq:antisymmetry}
\end{equation}

\bigskip

\noindent To account both for antisymmetry and indistinguishability, John C. Slater considered all possible distributions of electrons in the one-electron wave functions, conveniently expressed via a determinant, the \textit{Slater determinant}~\cite{Slater1929}, as shown in~\ref{eq:slaterDeterminant}.

\begin{equation}
  |\Psi^{n}\rangle \approx \frac{1}{\sqrt{n!}}\begin{vmatrix}
    \psi_1(r_1) & \psi_2(r_1) & \cdots & \psi_n(r_1) \\
    \psi_1(r_2) & \psi_2(r_2) & \cdots & \psi_n(r_2) \\
    \vdots & \vdots & \ddots & \vdots \\
    \psi_1(r_n) & \psi_2(r_n) & \cdots & \psi_n(r_n) \\
  \end{vmatrix}
  \label{eq:slaterDeterminant}
\end{equation}

\bigskip

\noindent Where the factorial term accounts for normalization and each entry in the determinant represents a one electron wave function, with an electron at position $r_i$.
This determinant represents a superposition of all possible Hartree products and, due to the nature of determinants, inherently accounts for sign-changes upon transposition of electrons, i.e.~antisymmetry.
Further, as one-electron functions must remain orthonormal, the Kronecker delta function $\delta_{ij}$ is employed as in~\ref{eq:kronecker}, ensuring orthogonality and further simplifying calculations by reducing the number of non-zero integrals to be computed.



\begin{equation}
  \langle\psi_i|\psi_j\rangle = \delta_{ij}\begin{cases}
    1 & \text{if } i = j \\
    0 & \text{if } i \neq j
  \end{cases} 
  \label{eq:kronecker}
\end{equation}

\bigskip

\subsubsection{LCAO-Approach}
Molecular orbitals, as they are employed in Slater determinants, are usually constructed via the \textit{Linear Combination of Atomic Orbitals} (LCAO) approach~\cite{Slater1954}.
In this approach, n atomic orbitals, ideally analytical eigensolutions of hydrogen-like systems, are combined linearly to n molecular orbitals as in~\ref{eq:lcao}, which in turn make up the Slater determinant.

\begin{equation}
  |\psi_i\rangle = \sum_{r}^{}c_{r i}|\phi_{r}\rangle
  \label{eq:lcao}
\end{equation}

\bigskip

\noindent Where $|\phi_{r}\rangle$ are the atomic orbitals, $c_{r i}$ the expansion coefficients and $|\psi_i\rangle$ the molecular orbitals.
During energy optimization according to the variational principle, these coefficients $c_{r i}$ are adjusted to achieve plausible approximations while using Lagrange multipliers to ensure orthonormality of the resulting molecular orbitals~\cite{sherrill2000introduction}.
Due to their computational effort, albeit high accuracy, the analytical Slater type orbitals (STO) of hydrogen are often foregone in favor of simpler Gaussian type orbitals (GTO), several of which may be contracted (CGTO) to approximate a single Slater orbital~\cite{Boys1950}. 
The level of contraction, number of CGTOs per atomic orbital as well as parametres like zeta-factors, are collected in basis sets, highly varied in their complexity and accuracy~\cite{Huzinaga1985}.

\subsubsection{Bloch's Theorem}
In periodic systems, defined by a unit cell, both the potential energies and wave functions at any given point are subject to periodicity.
As per \textit{Bloch's theorem}, utilizing this periodicity, one may express wave functions via periodically modulated plane waves to good effect~\cite{Bloch1929}.
This theorem is particularly useful when analyzing periodic systems, e.g.~crystals, allowing to construct wave functions or electronic states via Bloch functions, as in equation~\ref{eq:bloch}.

\begin{equation}
  \psi_{k}(r) = e^{ikr}u_{k}(r)
  \label{eq:bloch}
\end{equation}

\bigskip

\noindent Where $k$ is the wave vector, $r$ the position and $u_{k}(r)$ a periodic function mirroring the periodicity of the unit cell.
Since the wave vector $k$ is not uniquely defined, it is usually restricted to the first Brillouin zone, the smallest unit cell in reciprocal space, avoiding redundancy~\cite{Ashcroft1976-ra}.

\subsubsection{Self-Consistent Field Method}
With approximations and formalisms in place, a suitable Slater determinant may now be constructed by way of adjusting the expansion coefficients $c_{r i}$ in the LCAO approach.
To this end, an operator $\hat{F}$ is introduced, the \textit{Fock operator}, describing the kinetic energy of an electron as well as its interaction with all other electrons in the system in a mean field approximation.
This formalism is shown in equation~\ref{eq:fockOperator}, with the molecular orbitals shown in their linearly decomposed form.

\begin{equation}
  \hat{F}\sum_{r}^{}c_{r i}|\phi_{r}\rangle = \epsilon_i\sum_{r}^{}c_{r i}|\phi_{r}\rangle
  \label{eq:fockOperator}
\end{equation}

\bigskip

\noindent Where $\epsilon_i$ is the energy eigenvalue of the $i$-th molecular orbital. Utilizing vector notation, this expression may be written as in~\ref{eq:fockOperator2}, resulting in what is known as the \textit{Roothan-Hall equation}.


\begin{equation}
  \mathbf{Fc} = \mathbf{Sc}\mathbf{E}
  \label{eq:fockOperator2}
\end{equation}

\noindent Where $\mathbf{F}$ is the Fock matrix containing all possible Fock operators, $\mathbf{c}$ the coefficent matrix $\mathbf{S}$ the overlap matrix and $\mathbf{E}$ the orbital energy matrix.
Should the basis sets used be orthonormal, as enforced by the minimization constraints, the overlap matrix $\mathbf{S}$ reduces to the identity matrix, simplifying the Roothan-Hall equation to~\ref{eq:fockOperator3}.

\begin{equation}
  \mathbf{Fc} = \mathbf{Ec}
  \label{eq:fockOperator3}
\end{equation}

\bigskip

\noindent Due to the Fock operator containing the molecular orbitals, which themselves depend on the chosen coefficients, the Roothan-Hall equation is solved via an iterative prodecure, where an initial set of coefficients is chosen before the first iteration.
Then, the Fock matrix is constructed and diagonalized, yielding a new set of coefficients as well as energy eigenvalues.
The latter then serve as criteria for convergence, with iterations being carried out until the difference in energy is below a predefined threshold.
The so received approximated wavefunction may then be used for further analysis of the system, e.g.~calculating forces or charges acting upon atoms.

\subsubsection{Post Hartree-Fock Methods}
Hartree-Fock, mainly due to employing only a single Slater determinant, neglects part of electron correlation, i.e.~how electrons in movement are affected by the presence of other electrons.
While some correlation is included in the exchange terms of the Fock operator, neglecting other forms of correlation causes the method to be limited to a theoretical \textit{Hartree-Fock limit}, the lowest energy achievable.
Over the years, many methods were developed to account for said correlation, congregating under the term \textit{Post Hartree-Fock methods}.
One such method is \textit{Configuration Interaction} (CI), where the HF wave function is expanded to include excited determinants, which are then combined linearly to approximate a correlated wave function. Just like in base HF, the vartiational principle is applied when weighing the contribution of excited states.
Including all possible excited states, referred to as \textit{Full Configuration Interaction} (Full-CI), while providing the most accurate results, is computationally unfeasible for most molecules, which is why truncation is usually applied.

\subsection{Density Functional Theory}
Spawning from the field of solid-state physics, \textit{Density Functional Theory} (DFT) is a quantum mechanical method utilizing an alternative approach to many-electron systems with the aim of addressing the inaccuracies of HF and the high computational cost of post-HF methods.
Where wave-functions depend on 3N spatial coordinates, excluding spin, DFT instead focuses on the electron density $\rho(\vec{r})$ of a system, a property defined by only 3 spatial coordinates, theoretically decreasing computational demand drastically~\cite{Orio2009}.
The formalisms and qualitative discussions in the following subsections~\ref{hohenbergKohn} trough~\ref{hybrid} are based on \textit{A Chemists's Guide to Density Functional Theory} by Koch and Holthausen~\cite{Koch2001-yq}.

\subsubsection{Hohenberg-Kohn Theorems}\label{hohenbergKohn}
At the heart of DFT lie two theorems, formulated by Pierre Hohenberg and Walter Kohn in 1964~\cite{Hohenberg1964}.
First, the ground-state of a many electron system, and by extension its properties, are uniquely determined by its electron density $\rho(\vec{r})$. In short, the external potential of a system is a unique functional of the electron density.
Secondly, such a functional, if applied to the ground-state density, will yield the lowest possible energy of the system and this ground-state density corresponds to the exact solution of the Schrödinger equation. 
This, in essence, is the variational principle applied to the electron density functionals, as shown in equation~\ref{eq:hohenbergKohn}.

\begin{equation}
  E[\rho_{Trial}(\vec{r})] \geq E[\rho_{0}(\vec{r})]
  \label{eq:hohenbergKohn}
\end{equation}



\subsubsection{Kohn-Sham Equations}\label{kohnSham}

The \textit{Kohn-Sham} approach (KS), as derived by Kohn and Sham in 1965~\cite{Kohn1965}, forms the basis of current DFT approaches. 
In this approach, a system of non-interacting electrons yielding the electron density of the original problem is constructed as seen in equation~\ref{eq:kohnSham}.

\begin{equation}
  \rho_0(\vec{r}) = \sum_{i}^{N}|\phi_i(\vec{r})|^2
  \label{eq:kohnSham}
\end{equation}

\bigskip

\noindent Ironically, the wavefunction of this non-interacting system is represented by a Slater determinant, referred to as a Kohn-Sham determinant in the context of DFT.
The total energy of the system is then expressed as shown in equation~\ref{eq:kohnSham2}.

\begin{equation}
  E[\rho(\vec{r})] = T_s[\rho(\vec{r})] + J[\rho(\vec{r})] + E_{XC}[\rho(\vec{r})]
  \label{eq:kohnSham2}
\end{equation}

\bigskip

\noindent With $T_s$ being the kinetic energy of the non-interacting system, $J$ the Coulomb energy and $E_{XC}$ the exchange-correlation energy.
While the kinetic energy is for the most part comparatively easy to solve, the exchange-correlation functionals, which also include a "true" part of the kinetic energy, for such a system are unknown, leading to many DFT approaches with the sole purpose of defining increasingly accurate functionals.
Should such an explicit and exact form of the exchange-correlation functional $E_{XC}[\rho(\vec{r})]$ and its corresponding potential $V_{XC}[\rho(\vec{r})]$ be found, the Kohn-Sham equations would, in principal, be exact solutions to many body problems.

\subsubsection{Local Density Approximation}
Among the simplest methods in DFT, \textit{Local (Spin) Density Approximation (L(S)DA)} employs the electron density as a homogenous electron gas.
While effective in describing the solid metals and alloys it was designed for, LDA and its open-shell variant LSDA are considered inadequate for systems involving variable electron densities like MOFs~\cite{Perdew1981}.
In practice, to simplify calculations, the exchange and correlation functionals applied onto the electron density, as shown in equation~\ref{eq:lda}, are considered seperable.

\begin{equation}
  E_{XC}^{LDA}[\rho(\vec{r})] = \int_{-\infty}^{\infty} \epsilon_{XC}[\rho(\vec{r})]\rho(\vec{r})d\vec{r} = \int_{-\infty}^{\infty} \epsilon_{X}[\rho(\vec{r})]\rho(\vec{r})d\vec{r} + \int_{-\infty}^{\infty} \epsilon_{C}[\rho(\vec{r})]\rho(\vec{r})d\vec{r}
  \label{eq:lda}
\end{equation}

\bigskip

\noindent Where $\epsilon_{XC}[\rho(\vec{r})]$ is the exchange-correlation energy per particle and $\rho(\vec{r})$ the electron density, with square brackets denoting a functional dependence.
Since the exchange-energy density of a homogenous electron gas is known analytically, derived by Bloch and Dirach as early as 1929~\cite{Bloch1929}\cite{Dirac1930}, LDA approaches apply this result pointwise to a non-homogenous density to calculate exchange-energy as in equation~\ref{eq:lda2}~\cite{parr1994density}.

\begin{equation}
  E_X^{LDA}[\rho(\vec{r})] = -\frac{3}{4}\left(\frac{3}{\pi}\right)^{1/3}\int_{-\infty}^{\infty}\rho(\vec{r})^{4/3}d\vec{r}
  \label{eq:lda2}
\end{equation}

\bigskip

\noindent While exchange-energy may be explicit, the correlation-energy of a homogeneous electron gas is not, leading to various expressions being derived over time, with e.g~\textit{Monte Carlo} approaches providing accurate results~\cite{Ceperley1980}.

\subsubsection{Generalized Gradient Approximation}
The \textit{Generalized Gradient Approximation} (GGA) differs from LDA by employing not only the electron density $\rho(\vec{r})$, but also its gradient $\nabla\rho(\vec{r})$ with respect to spatial coordinates as in equation~\ref{eq:gga2}.
Functionals utilizing both the electron density and its gradient once more assume a seperation of the exchange and correlation functionals, as shown in equation~\ref{eq:gga1}.

\begin{equation}
  E_{XC}^{GGA}[\rho(\vec{r}), \nabla\rho(\vec{r})] = \int_{-\infty}^{\infty} \epsilon_{XC}[\rho(\vec{r}), \nabla\rho(\vec{r})]\rho(\vec{r})d\vec{r} \\
\label{eq:gga2}
\end{equation}


\begin{equation}
  E_{XC}^{GGA} = E_X^{GGA} + E_C^{GGA}
  \label{eq:gga1}
\end{equation}

\bigskip

\noindent The gradient corrected exchange functional of this approach may be written as in equation~\ref{eq:gga3}.

\begin{equation}
  E_X^{GGA} = E_X^{LDA} - \int_{-\infty}^{\infty} F(s_{\sigma}) \rho_{\sigma}^{4/3}(\vec{r})d\vec{r}
  \label{eq:gga3}
\end{equation}

\bigskip

\noindent Where the argument for the function F is the reduced gradient $s_{\sigma}$ for spin $\sigma$, understood as a local inhomogenity parameter and calculated as in equation~\ref{eq:inhomogenity}.
The value of $s_{\sigma}$ is large for regions of small densities such as those far from the nuclei and small for bonding regions or regions of large density.
For the function F, two main realizations are employed. The first one, based on a GGA functional developed by Becke in 1988~\cite{Becke1988}, approaches inhomogenity as in equation~\ref{eq:gga4}.

\begin{equation}
  s_{\sigma} = \frac{|\nabla\rho_{\sigma}(\vec{r})|}{\rho_{\sigma}^{4/3}(\vec{r})}
  \label{eq:inhomogenity}
\end{equation}

\begin{equation}
  F^{B} = \frac{\beta s^2_{\sigma}}{1 + 6\beta s_{\sigma} \sinh^{-1}s_{\sigma}}
  \label{eq:gga4}
\end{equation}

\bigskip

\noindent In this functional, abbreviated simply as B or B88, the parameter $\beta$ has been empirically determined to a value of 0.0042 by fitting to known exchange energies.
The second class of functionals laying much of the groundwork for modern approaches employ F as a rational function of $s_{\sigma}$, as examplified \textit{via} Perdews's exchange functional P86 in equation~\ref{eq:gga5}~\cite{Perdew1986}.

\begin{equation}
  F^{P86} = \Biggl(1+1.296\biggl(\frac{s_{\sigma}}{(24\pi^2)^{1/3}}\biggl)^2 + 14\biggl(\frac{s_{\sigma}}{(24\pi^2)^{1/3}}\biggl)^4 + 0.2\biggl(\frac{s_{\sigma}}{(24\pi^2)^{1/3}}\biggl)^6 \Biggl)^{1/15}
  \label{eq:gga5}
\end{equation}

\bigskip

\noindent Here, as opposed to the Becke functional, no empirical parameters are present.
With exchange functionals established, gradient-corrected correlation functionals are a different beast entirely when it comes to analytical complexity.
One widely used functional is the P or P86 correlation functional, a counterpart to the 1986 Perdew exchange functional discussed previously, employing an empirical parameter fitted to the correlation energy of a neon atom~\cite{Perdew1986_2}.
Functionals free of empirical parameters have also been developed, such as \textit{PW91} by Perdew and Wang in 1991~\cite{Burke1998}.
Finally and most notably, Lee, Yang and Parr formulated their \textit{LYP} functional in 1988, derived from the \textit{Colle-Salvetti} correlation energy expression~\cite{Lee1988}, in which the correlation energy density is expressed in terms of the electron density as well as a Laplacian of the second-order HF density matrix~\cite{Colle1975}.
While freely combining different exchange and correlation functionals is generally possible, only a small subset of pairings is seeing much use in modern theoretical chemistry, with the combination of Becke's exchange functional and LYP correlation functional, known as BLYP, being one of the most widely used frameworks.


\subsubsection{Hybrid Functionals}\label{hybrid}

In an attempt to further improve the accuracy of DFT calculations, \textit{Hybrid Functionals} utilize a combination of the exact Hartree-Fock exchange energies and approximated correlation functionals, as shown in equation~\ref{eq:hybrid}.

\begin{equation}
  E_{XC} = E_X^{HF} + E_C^{KS}
  \label{eq:hybrid}
\end{equation}

\bigskip

\noindent However, attempting to exploit the strengths of both HF and DFT using reckless combination provided no benefit over previously established approaches, with inaccuracies ranging in the high two-digit $kcal/mol$ range especially when applie to molecules and chemical bonding.
The first succesful hybrid functional was derived by Becke in 1993~\cite{Becke1993_hybrid}, employing a combination of HF exchange and DFT correlation in the `\textit{half-and-half}' approach with already promisingly low average errors.
In the next iteration, ultimately leading to arguably the most widely used hybrid functional, Becke introduced semiempirical coefficients to weigh the various hybrid components~\cite{Becke1993a}. 
This combination of HF exchange with local and gradient-corrected correlation and exchange terms is shown in equation~\ref{eq:becke1}, with the semiempirical parameters $a=0.20$, $b=0.72$ and $c=0.81$ fitted to optimally reproduce previous experimental results. 

\begin{equation}
  E_{XC}^{B3} = E_{XC}^{LSD}+a(E_{XC}^{\lambda = 0}-E_{X}^{LSD})+bE_{X}^{B}+cE_{C}^{PW91}
  \label{eq:becke1}
\end{equation}

\bigskip

\noindent Where $\lambda=0$ denotes an interaction free system and $\lambda=1$ a fully interacting system.
Suggested by Stephens et al.~in 1994, replacing the PW91 correlation functional in equation~\ref{eq:becke1} with a LYP functional, the B3LYP exchange-correlation functional was born~\cite{stephens_ab_1994}, as seen in equation~\ref{eq:b3lyp}.

\begin{equation}
  E_{XC}^{B3LYP} = (1-a) E_{X}^{LSD}+aE_{XC}^{\lambda = 0}+bE_{X}^{B88}+cE_{C}^{LYP}+(1-c)E_C^{LSD}
  \label{eq:b3lyp}
\end{equation}

\bigskip

\noindent Where the values for the parameters a, b and c are directly taken from Becke's original functional.
The B3LYP functional showed promising results, with unsigned errors of only slightly above 2 $kcal/mol$ in respect to a Gaussian-2 reference set, comprised of corrected MP2 calculations~\cite{Curtiss1991}.
Although B3LYP remained a pillar of density functional theory, hybrid functionals saw many more iterations and improvements over the years, with notable examples being the PBE0 model by Adamo and Barone in 1999~\cite{Adamo1999} or the M06 family of functionals by Truhlar and Zhao in 2007~\cite{Zhao2007}.

\subsubsection{Dispersion Corrections}
The following discussions are sourced from a review by Klimeš and Michaelides, published in 2012~\cite{Klime2012}.
\\ \par \noindent One of the main shortcomings of DFT is inherent to the approximations made for XC functionals, namely the inability of most standard functionals to properly describe long-range electron correlation, colloquially referred to as \textit{van der Waals} forces.
Dispersion, in essence the attractive interaction between instantaneous dipoles, is well described as decaying with the sixth power of the distance between two particles.
However, most standard exchange-correlation functionals do not accurately describe this dispersion because instantaneous fluctuations in density are not considered and because only local properties are considered for calculating most XC energies.
These functionals only model binding or repulsion when there is an overlap of the electron densities between two atoms, and since this overlap decreases exponentially with distance, the $-1/r^6$ decay is not accurately modelled.
\\ \par \noindent To counteract this deficit, a number of dispersion correction schemes with varying levels of complexity have been developed, two of which will be discussed in detail.
Among the simplest schemes are density functionals that are specifically fitted in such a way as to reproduce weak interaction around minima, like the \textit{Minnesota functionals} by Zhao and Truhlar~\cite{Zhao2007-oa}.
While still suffering from incorrect asymptotic behaviour, owing to the reference data used for fitting, these functionals can also represent other properties than weak interaction like e.g.~reaction barriers, making them at least somewhat accurate for a wide range of generic problems.
At around the same level of complexity, one may include pseudopotential projection to model dispersion.
While laborious to fit for each element, methods such as the \textit{dispersion corrected atom-centered potentials} (DCACP)~\cite{vonLilienfeld2004} or \textit{local atomic potentials} (LAP)~\cite{Sun2008} have shown promising results.
\\ \par \noindent At the next level of complexity, an additional energy term is added to the DFT energy, accounting for missing long range interaction as in equation~\ref{eq:disp1}, with the dispersion interaction calculated as in equation~\ref{eq:disp2}.

\begin{equation}
  E_{Tot} = E_{DFT} + E_{disp}
  \label{eq:disp1}
\end{equation}

\begin{equation}
  E_{disp} = - \sum_{A,B}\frac{C_{6}^{AB}}{r^6_{AB}}
  \label{eq:disp2}
\end{equation}

\bigskip

\noindent Where $C_{6}^{AB}$ is the dispersion coefficient between atoms A and B and $r_{AB}$ the distance between them.
Generally referred to as \textit{DFT-D}, these methods assume dispersion to be pairwise additive over all elemental pairs A and B.
While computationally inexpensive and therefore widely used, DFT-D methods are limited by their disregard for many-body dispersion as well as the influence of chemical environments and different states, since the coefficents $C_{6}^{AB}$ are kept constant.
Additionally, calculating these constant coefficients often requires experimental input, limiting the number of treatable elements.
Presented by Grimme in 2006, the \textit{DFT-D2} schemes employ a formula coupling ionization potentials with static polarizabilities to calculate dispersion coefficients for many common elements~\cite{Grimme2006}.
Despite their shortcomings, especially for alkali and alkali earth atoms, DFT-D2 methods are among the most widely used dispersion corrections in modern DFT usage.
\\ \par \noindent Generally, DFT-D correction schemes diverge at short distances and must therefore be `damped', as in equation~\ref{eq:disp3}.

\begin{equation}
  E_{disp} = - \sum_{A,B}\frac{C_{6}^{AB}}{r^6_{AB}}f(r_{AB},A,B)
  \label{eq:disp3}
\end{equation}

\bigskip

\noindent Where $f(r_{AB},A,B)$ is a damping function equaling to one for large distances and decreasing the dispersive energy to zero or a constant for small distances.
To avoid causing additional divergence, these damping functions must be carefully adjusted to the chosen XC functional.
\\ \par \noindent Moving further up the `stairway' of complexity, one may employ so called \textit{non-local} correlation functionals, which add non-local correlations to local correlation functionals and do not rely on external experimental data, with e.g.~the \textit{non-local van der Waals density functional} (vdW-DF)~promising high accuracies~\cite{Klime2009_vdW-DF}.
Going beyond even these methods, the assumption of pairwise additivity must be left behind and many-body dispersion considered, though these methods would be too complex to elaborate on in the scope of this thesis.

\subsubsection{Density Functional Tight Binding}
The following derivations are based on a review by Elstner et al.~titled \textit{Density functional tight binding} and published in 2014~\cite{Elstner2014-zp}.
\\ \par \noindent Despite the many advancements in DFT and every increasing chemical accuracy, faster and computationally less demanding methods are always subject of research with the goal of reaching longer timeframes of simulation or upscale the size of studied systems.
Building on the principles of \textit{tight-binding} theories~\cite{Goringe1997},  \textit{Density Functional Tight Binding} (DFTB) is one such method.
DFTB may be understood as a Taylor series expansion of the Kohn-Sham density functional total energy, as previously shown in its contracted form in equation~\ref{eq:kohnSham} of chapter~\ref{kohnSham}, around a reference density $\rho_0(\vec{r})$.
To forego finding the electron density $\rho(\vec{r})$ that minimizes the energy expectation value, this reference density is perturbed by a density fluctuation as described in equation~\ref{eq:dftb1}.

\begin{equation}
  \rho(\vec{r}) = \rho_0(\vec{r}) + \delta\rho(\vec{r})
  \label{eq:dftb1}
\end{equation}

\bigskip

\noindent Expanding the exchange-correlation energy part of the KS energy in a Taylor series up to the third order leads to the different terms shown in equations~\ref{eq:term1},~\ref{eq:term2},~\ref{eq:term3} and~\ref{eq:term4}.

\begin{equation}
  E^{0}[\rho_0] = \frac{1}{2}\sum_{ab}^{}\frac{Z_a Z_b}{R_{ab}} - \frac{1}{2} \int\int \frac{\rho_0(\vec{r})\rho_0(\pvec{r}')}{|\vec{r}-\pvec{r}'|}d\vec{r}d\pvec{r}' - \int V^{XC}[\rho_0]\rho_0(\vec{r})d\vec{r}+E^{XC}[\rho_0] \\
  \label{eq:term1}
\end{equation}
\begin{equation}
  E^1[\rho_0, \delta\rho] = \sum_{i}^{}n_i \langle\psi_i|\hat{H}[\rho_0]|\psi_i\rangle
  \label{eq:term2}
\end{equation}
\begin{equation}
  E^2[\rho_0, (\delta\rho)^2] = \frac{1}{2}\int\int\Biggl(\frac{1}{|\vec{r}-\pvec{r}'|}+\frac{\delta^2E^{XC}[\rho]}{\delta\rho(\vec{r})\delta\rho(\pvec{r}')}\Biggr|_{\rho_0}\Biggr)\delta\rho(\vec{r})\delta\rho(\pvec{r}')d\vec{r}d\pvec{r}'
  \label{eq:term3}
\end{equation}
\begin{equation}
  E^3[\rho_0, (\delta\rho)^3] = \frac{1}{6}\int\int\int\frac{\delta^3E^{XC}[\rho]}{\delta\rho(\vec{r})\delta\rho(\pvec{r}')\delta\rho(\pvec{r}'')}\Biggr|_{\rho_0}\delta\rho(\vec{r})\delta\rho(\pvec{r}')\delta\rho(\pvec{r}'')d\vec{r}d\pvec{r}'d\pvec{r}''
  \label{eq:term4}
\end{equation}

\bigskip

The entire third order expansion may then be written as in equation~\ref{eq:taylor}

\begin{equation}
  E[\rho] = E^{0}[\rho_0] + E^1[\rho_0, \delta\rho] + E^2[\rho_0, (\delta\rho)^2] + E^3[\rho_0, (\delta\rho)^3]
  \label{eq:taylor}
\end{equation}

\bigskip

\noindent In DFTB, the zeroeth order energy term $E^0$ is determined only for the reference system, allowing transferability to different chemical environments.
It is approximated by a sum of pair potentials referred to as the repulsive energy term, as shown in equation~\ref{eq:repulsive}, and either fitted to DFT calculations or to emprical data.

\begin{equation}
  E^{0}[\rho_0] \approx E_{rep} = \frac{1}{2} \sum_{ab}^{}V_{rep}^{ab}
  \label{eq:repulsive}
\end{equation}



\subsection{Molecular Dynamics Simulations}
Alongside advancements in the field of digital electronics came the possibilities of simulating chemical 
systems on ever larger scales. Where practical experiments are limited by equipment, time, resources and safety regulations, 
simulations can be conducted cost effectively and with an ever increasing level of precision.
Simulating a chemical system to receive an ensemble of configurations starts with choosing the most viable
theoretical framework. Molecular Dynamics (MD), utilizing the DFTB method, is one of those frameworks particularly well suited for  
the theoretical time-dependant analysis of periodic chemical systems. 

\bibliography{thesis}

\end{document}