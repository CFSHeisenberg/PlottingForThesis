%Preamble
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{setspace}
\setstretch{1.5}
\usepackage{geometry}
\geometry{a4paper, portrait, margin=30mm, bmargin=30mm, tmargin=30mm}
\usepackage{amsmath}
\usepackage{esvect}
\usepackage[hidelinks]{hyperref}

\newcommand{\pvec}[1]{\vec{#1}\mkern2mu\vphantom{#1}}

%Bibliography
%\usepackage[backend = biber, style = alphabetic, sorting = ynt]{biblatex}
%\addbibresource{thesis.bib}
\bibliographystyle{unsrt}

%Patch abstract command to make it normal size
\makeatletter
\renewenvironment{abstract}{%
    \if@twocolumn
      \section*{\abstractname}%
    \else %% <- here I've removed \small
      \begin{center}%
        {\bfseries \Large\abstractname\vspace{\z@}}%  %% <- here I've added \Large
      \end{center}%
      \quotation
    \fi}
    {\if@twocolumn\else\endquotation\fi}
\makeatother


\begin{document}
\begin{center}
\thispagestyle{empty}
\large{\textbf{Univserity of Innsbruck}}\\[-0.9ex]
\large{Department of General, Inorganic and Theoretical Chemistry}\\
\vspace{0.3cm}
\begin{center}
\includegraphics[width=7cm]{Images/Logo.jpg}\\
\vspace{0.9cm}
\textbf{\LARGE{Master Thesis}}
\medskip\par
\vspace{1.2cm}
\Large{\textbf{Structure and Thermodynamics of \\ Guest@MOF Material}}\\[-0.5ex]
\vspace*{1.5cm}
\large{Investigating GUEST in MIL-68Ga via a Molecular Dynamics Simulation Approach}\\[-1.5ex]
\vspace*{1.5cm}
\bigskip\par
\textbf{Michael Helmut Fill, BSc. }\\[-1ex]
\medskip
\textbf{Supervisor:} Assoc. Prof. Dr. Thomas Hofer\\
Hier Datum einfügen
\end{center}
\end{center}

\newpage

\begin{abstract}
  This thesis is dedicated to the brave Mujahideen fighters of Afghanistan.
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}

\section{Theory}
\subsection{Quantum Chemistry}
At the dawn of the 20th century, general consensus among many physicists versed in classical mechanics was that the
fundamental laws of nature had been solved, and that the by then still unsolved problems of e.g.~black body radiation
would be resolved in due time. However, when Max Planck published his ideas on the quantization of energy in 1900~\cite{Planck1901}, and especially
after Albert Einstein realistically explained the photoelectric effect using Plancks' hypothesis in 1905~\cite{Einstein1905}, it became clear that the classical laws of physics were 
inadequate at explaining the behavior of matter on the atomic scale.
The field of quantum mechanics was born, and together with further groundbreaking contributions by, among others, Niels Bohr~\cite{Bohr1913}, Werner Heisenberg~\cite{Heisenberg1927} and Louis de Broglie~\cite{Broglie1924},
paved the way for Erwin Schrödinger to lay the groundwork for quantum chemistry with his wave equation in 1926~\cite{Schrdinger1926}\cite{Schrdinger1926-2}.
\subsubsection{The Schrödinger Equation}
Starting from the initial assumption that all properties of a given system could be described by a wave function $\Psi$, Schrödinger derived his now well-known
time-independant~(\ref{eq:schrodingerIndependant}) and, more general, time-dependant~(\ref{eq:schrodingerDependant}) equations.


\begin{equation}
  \hat{H}\Psi = E\Psi
  \label{eq:schrodingerIndependant}
\end{equation}
\begin{equation}
  i\hbar\frac{\partial}{\partial t}\Psi = \hat{H}\Psi
  \label{eq:schrodingerDependant}
\end{equation}

\bigskip

\noindent With $\Psi$ being the eigenfunction of the system, $\hbar$ the reduced Planck constant, $\hat{H}$ the Hamiltonian operator, $t$ the time and $E$ the energy of the system, or eigenvalue of the Hamiltonian operator.
The Hamiltonian operator~(\ref{eq:hamiltonian}) is defined as the sum of the kinetic and potential energy operators, which, when applied to a state function $\Psi$, yields the total energy of the system as its eigenvalue to the eigenfunction $\Psi$.

\begin{equation}
  \hat{H} = -\frac{\hbar^2}{2m}\nabla^2 + E_{pot} = \hat{T} + \hat{V}
  \label{eq:hamiltonian}
\end{equation}

\bigskip

\noindent Here, $\nabla^2$ is the Laplace operator, $m$ the mass of the particle and $E_{pot}$ the potential energy of the system, with the kinetic energy and potential energy operators historically referred to as $\hat{T}$ and $\hat{V}$ respectively.
For a single particle with mass $m$ moving in three-dimensional space, the time independant Schrödinger equation can then be written as~(\ref{eq:differential}).

\begin{equation}
  -\frac{\hbar^2}{2m}\nabla^2\Psi + E_{pot}\Psi = E\Psi
  \label{eq:differential}
\end{equation}

\begin{equation*}
  \nabla^2 = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}
\end{equation*}

\bigskip

\noindent This linear differential equation~(\ref{eq:differential}) forms the basis for determining the wave function of a given system. 
A wavefunction $\Psi$ that satisfies the Schrödinger equation and results in its respective eigenvalue $E$ must therefore be an eigenfunction to the Hamiltonian operator.
It is due to this that finding the correct wave function $\Psi$ for a system is referred to as an eigenvalue problem.\\
Hereby, it is important to note that a wavefunction is not directly observable and therefore no meaning can be attributed to its values.
However, the square modulus of the wavefunction $|\Psi|^2$, as interpreted by Max Born~\cite{Born1926}, is proportional to the observable probability density of an electron in the given system.

\subsubsection{Hamiltonian Operator}
As previously mentioned, the Hamiltonian operator is defined as the sum of kinetic and potential energies in a given system. 
The notation given in equation~\ref{eq:hamiltonian} is a simplification of the actual Hamiltonian operator, which can be written as in equation~\ref{eq:hamiltonian3} and~\ref{eq:hamiltonian2} as them sum of its individual terms, being the kinetic energy of the nuclei $\hat{T}_N$, the kinetic energy of the electrons $\hat{T}_e$, the potential energy of the nuclei $\hat{V}_{NN}$, the potential energy of the nuclei and electrons $\hat{V}_{Ne}$ and the potential energy of the electrons $\hat{V}_{ee}$.

\begin{equation}
  \hat{H} = \hat{T}_N + \hat{T}_e + \hat{V}_{NN} + \hat{V}_{Ne} + \hat{V}_{ee}
  \label{eq:hamiltonian3}
\end{equation}

\begin{equation}
  \begin{aligned}
    \hat{H} = & -\sum_{I=1}^{N}\frac{\hbar^2}{2m_I}\nabla_I^2 -\sum_{i=1}^{n}\frac{\hbar^2}{2m_e}\nabla_i^2 + \frac{1}{2} \sum_{I\neq J}^{N} \frac{q_{e}^2}{4\pi \epsilon_0}\frac{Z_I Z_J}{|r_I - r_J|} \\
    & - \sum_{I=1}^{N}\sum_{i=1}^{n}\frac{q_{e}^2}{4\pi \epsilon_0}\frac{Z_I}{|r_I - r_i|} + \frac{1}{2}\sum_{i\neq j}^{n}\frac{q_{e}^2}{4\pi \epsilon_0}\frac{1}{|r_i - r_j|}
  \end{aligned}
  \label{eq:hamiltonian2}
\end{equation}

\bigskip

\noindent With $N$ being the number of nuclei, $n$ the number of electrons, $m_I$ the mass of the $I$-th nucleus, $m_e$ the mass of the electron, $Z_I$ the charge of the $I$-th nucleus, $r_I$ the position of the $I$-th nucleus, $r_i$ the position of the $i$-th electron, $q_e$ the elementary charge and $\epsilon_0$ the vacuum permittivity.
To simplify this expression, atomic units are introduced, leading to the Hamiltonian as shown in equation~\ref{eq:hamiltonianAtomic}.

\bigskip

\begin{equation}
  \begin{aligned}
    \hat{H} = & -\sum_{I=1}^{N}\frac{1}{2m_I}\nabla_I^2 -\sum_{i=1}^{n}\frac{1}{2}\nabla_i^2 + \frac{1}{2} \sum_{I\neq J}^{N} \frac{Z_I Z_J}{|r_I - r_J|} \\
    & - \sum_{I=1}^{N}\sum_{i=1}^{n}\frac{Z_I}{|r_I - r_i|} + \frac{1}{2}\sum_{i\neq j}^{n}\frac{1}{|r_i - r_j|}
  \end{aligned}
  \label{eq:hamiltonianAtomic}
\end{equation}

\bigskip

\subsubsection{Dirac Notation}
Since it is not feasible and often impossible to solve the Schrödinger equation for systems of 3 or more unconstrained particles, should a coulombic potential be employed~\cite{Toli2019}, a number of approximations are made, some of which will be discussed in the following sections. 
To work with these approximated wave functions, the expressions for the Schrödinger equation must be adjusted as in~\ref{eq:dirac}. 

\begin{equation}
  \hat{H}\Psi = E\Psi
  \label{eq:dirac}
\end{equation}
\begin{equation*}
  \Psi^{*}\hat{H}\Psi = \Psi^{*}E\Psi
\end{equation*}
\begin{equation*}
  \int_{-\infty}^{\infty}\Psi^{*}\hat{H}\Psi d\tau = \int_{-\infty}^{\infty}\Psi^{*}E\Psi d\tau
\end{equation*}

\bigskip

\noindent With $\tau$ being the volume element of the system and $\Psi^{*}$ the complex conjugate of the wave function $\Psi$.
The received energy, granted that $\Psi$ is an approximated wave function, is now an estimate of the actual analytical energy.
To simplify these integral expressions the \textit{bra-ket} notation, also known as \textit{Dirac} notation~\cite{Dirac1939}, is employed as shown in equation~\ref{eq:dirac2}.

\begin{equation}
  \langle\Psi|\hat{H}|\Psi\rangle = \langle\Psi|E|\Psi\rangle
  \label{eq:dirac2}
\end{equation}

\bigskip

\subsubsection{Variational Principle}
With the framework for utilizing approximated wave functions in place, the next step is to qualify a chosen wave function according to its accurary as an approximation to the analytical wave function.
Since the energy of a system is constant, it can be taken out of the integral expression, leaving the remaining normalized integral yielding 1.
Choosing now a trial wave function $\Psi_{Trial}$ and applying the Hamiltonian operator to it, the resulting energy expectation value is always greater or equal to the ground state energy of the system, as shown in equation~\ref{eq:variational}.
This method of finding parametres to construct the best possible approximated wave function is called the variational method and sits at the heart of many quantum mechanical approaches~\cite{Griffiths2017}.

\begin{equation}
  \langle\Psi_{Trial}|\hat{H}|\Psi_{Trial}\rangle = \langle E\rangle \geq E_0
  \label{eq:variational}
\end{equation}

\bigskip

\subsection{Hartree-Fock Theory}
As previously mentioned, the exact solution to the Schrödinger equation for systems containing more than 2 unconstrained particles is analytically not feasible.
To this end, several approximations are employed to simplify the eigenvalue problem down to an analytically solvable form.
The Hartree-Fock method employs such approximations and provides a fundamental framework for the analysis of many-electron systems.

\subsubsection{Non-relativistic Stationary Systems}
An important assumption not exclusive to Hartree-Fock Theory is the non-relativistic nature of the analysed system, 
implying stationary systems with velocities much smaller than the speed of light.
This assumption allows one to employ only the much simpler time-independant Schrödinger equation, with the trade-off being that effects arising from special relativity, e.g.~spatial contraction or time dilation in heavy atoms, are not accounted for.
Additionally, since only stationary states are computed, the analysis of time-dependant processes require specific simulation frameworks, one of which will be discussed at a later point.

\subsubsection{Born-Oppenheimer Approximation}
The Hartree-Fock method inherently assumes the Born-Oppenheimer approximation, which states that the wave functions of nuclei and electrons of a given system may be decoupled~\cite{Born1927}.
This seperation is justified via the large mass difference of a factor of 1836 between electrons and protons, and allows for the assumption that nuclei may be considered stationary during the calculation of the electronic wave functions.

\begin{equation}
  |\Psi^{n, N}\rangle = |\Psi^{n}\rangle|\Psi^{N}\rangle 
\end{equation}

\bigskip

\noindent With $n$ and $N$ denoting the electronic and nuclear wave functions respectively. As a result of this approximation,
the term for the kinetic energy of the nuclei in the Hamiltonian is omitted, while the coulombic repulsion between different nuclei becomes a constant term, further simplifying calculations.
An eigenfunction to this electric Hamiltonian, as shown in~\ref{eq:electricHamiltonian},  is now referred to as an electronic eigenfunction to the Schrödinger equation.

\begin{equation}
  \hat{H} = -\sum_{i=1}^{n}\frac{1}{2}\nabla_i^2 -\sum_{I=1}^{N}\sum_{i=1}^{n}\frac{Z_I}{|r_I - r_i|} + \frac{1}{2}\sum_{i\neq j}^{n}\frac{1}{|r_i - r_j|}  \label{eq:electricHamiltonian}
\end{equation}

\bigskip

\noindent Neglecting the quantum character of nuclei in this way renders the electronic Hamiltonian useless for describing quantum mechanical phenomena involving nuclei, yet allows for vastly increased computational efficiency.

\subsubsection{Independant Particle approximation}
Douglas R. Hartree provided a further approximation to n-electron wave functions in 1928 in the form of a product of independant one-electron functions, referred to as \textit{Hartree product}~\cite{Hartree1928}.
Equation~\ref{eq:hartreeProduct} shows this formalism, where each independant electrons $i$ interaction with all other $n-1$ electrons of the system is averaged out, referred to as a \textit{mean field approximation}.

\begin{equation}
  |\Psi^{n}\rangle = \prod_{i=1}^{n}|\psi_i\rangle
  \label{eq:hartreeProduct}
\end{equation}

\subsubsection{Slater Determinant}
While providing a good approximation to n-electron wavefunctions, the Hartree product does not account for the antisymmetrical nature of fermions due to their half-integer spin resulting from the Pauli exclusion principle~\cite{Pauli1925}.
In essence, this means that transposing any two, by definition indistinguishable, electrons in a wave function must result in a sign change. For a two particle system with coordinates $r_1$ and $r_2$, this may be written as seen in~\ref{eq:antisymmetry}.

\begin{equation}
  \Psi(r_1, r_2) = -\Psi(r_2, r_1)
  \label{eq:antisymmetry}
\end{equation}

\bigskip

\noindent To account both for antisymmetry and indistinguishability, John C. Slater considered all possible distributions of electrons in the one-electron wave functions, conveniently expressed via a determinant, the \textit{Slater determinant}~\cite{Slater1929}, as shown in~\ref{eq:slaterDeterminant}.

\begin{equation}
  |\Psi^{n}\rangle \approx \frac{1}{\sqrt{n!}}\begin{vmatrix}
    \psi_1(r_1) & \psi_2(r_1) & \cdots & \psi_n(r_1) \\
    \psi_1(r_2) & \psi_2(r_2) & \cdots & \psi_n(r_2) \\
    \vdots & \vdots & \ddots & \vdots \\
    \psi_1(r_n) & \psi_2(r_n) & \cdots & \psi_n(r_n) \\
  \end{vmatrix}
  \label{eq:slaterDeterminant}
\end{equation}

\bigskip

\noindent Where the factorial term accounts for normalization and each entry in the determinant represents a one electron wave function, with an electron at position $r_i$.
This determinant represents a superposition of all possible Hartree products and, due to the nature of determinants, inherently accounts for sign-changes upon transposition of electrons, i.e.~antisymmetry.
Further, as one-electron functions must remain orthonormal, the Kronecker delta function $\delta_{ij}$ is employed as in~\ref{eq:kronecker}, ensuring orthogonality and further simplifying calculations by reducing the number of non-zero integrals to be computed.



\begin{equation}
  \langle\psi_i|\psi_j\rangle = \delta_{ij}\begin{cases}
    1 & \text{if } i = j \\
    0 & \text{if } i \neq j
  \end{cases} 
  \label{eq:kronecker}
\end{equation}

\bigskip

\subsubsection{LCAO-Approach}
Molecular orbitals, as they are employed in Slater determinants, are usually constructed via the \textit{Linear Combination of Atomic Orbitals} (LCAO) approach~\cite{Slater1954}.
In this approach, n atomic orbitals, ideally analytical eigensolutions of hydrogen-like systems, are combined linearly to n molecular orbitals as in~\ref{eq:lcao}, which in turn make up the Slater determinant.

\begin{equation}
  |\psi_i\rangle = \sum_{r}^{}c_{r i}|\phi_{r}\rangle
  \label{eq:lcao}
\end{equation}

\bigskip

\noindent Where $|\phi_{r}\rangle$ are the atomic orbitals, $c_{r i}$ the expansion coefficients and $|\psi_i\rangle$ the molecular orbitals.
During energy optimization according to the variational principle, these coefficients $c_{r i}$ are adjusted to achieve plausible approximations while using Lagrange multipliers to ensure orthonormality of the resulting molecular orbitals~\cite{sherrill2000introduction}.
Due to their computational effort, albeit high accuracy, the analytical Slater type orbitals (STO) of hydrogen are often foregone in favor of simpler Gaussian type orbitals (GTO), several of which may be contracted (CGTO) to approximate a single Slater orbital~\cite{Boys1950}. 
The level of contraction, number of CGTOs per atomic orbital as well as parametres like zeta-factors, are collected in basis sets, highly varied in their complexity and accuracy~\cite{Huzinaga1985}.

\subsubsection{Bloch's Theorem}
In periodic systems, defined by a unit cell, both the potential energies and wave functions at any given point are subject to periodicity.
As per \textit{Bloch's theorem}, utilizing this periodicity, one may express wave functions via periodically modulated plane waves to good effect~\cite{Bloch1929}.
This theorem is particularly useful when analyzing periodic systems, e.g.~crystals, allowing to construct wave functions or electronic states via Bloch functions, as in equation~\ref{eq:bloch}.

\begin{equation}
  \psi_{k}(r) = e^{ikr}u_{k}(r)
  \label{eq:bloch}
\end{equation}

\bigskip

\noindent Where $k$ is the wave vector, $r$ the position and $u_{k}(r)$ a periodic function mirroring the periodicity of the unit cell.
Since the wave vector $k$ is not uniquely defined, it is usually restricted to the first Brillouin zone, the smallest unit cell in reciprocal space, avoiding redundancy~\cite{Ashcroft1976-ra}.

\subsubsection{Self-Consistent Field Method}
With approximations and formalisms in place, a suitable Slater determinant may now be constructed by way of adjusting the expansion coefficients $c_{r i}$ in the LCAO approach.
To this end, an operator $\hat{F}$ is introduced, the \textit{Fock operator}, describing the kinetic energy of an electron as well as its interaction with all other electrons in the system in a mean field approximation.
This formalism is shown in equation~\ref{eq:fockOperator}, with the molecular orbitals shown in their linearly decomposed form.

\begin{equation}
  \hat{F}\sum_{r}^{}c_{r i}|\phi_{r}\rangle = \epsilon_i\sum_{r}^{}c_{r i}|\phi_{r}\rangle
  \label{eq:fockOperator}
\end{equation}

\bigskip

\noindent Where $\epsilon_i$ is the energy eigenvalue of the $i$-th molecular orbital. Utilizing vector notation, this expression may be written as in~\ref{eq:fockOperator2}, resulting in what is known as the \textit{Roothan-Hall equation}.


\begin{equation}
  \mathbf{Fc} = \mathbf{Sc}\mathbf{E}
  \label{eq:fockOperator2}
\end{equation}

\noindent Where $\mathbf{F}$ is the Fock matrix containing all possible Fock operators, $\mathbf{c}$ the coefficent matrix $\mathbf{S}$ the overlap matrix and $\mathbf{E}$ the orbital energy matrix.
Should the basis sets used be orthonormal, as enforced by the minimization constraints, the overlap matrix $\mathbf{S}$ reduces to the identity matrix, simplifying the Roothan-Hall equation to~\ref{eq:fockOperator3}.

\begin{equation}
  \mathbf{Fc} = \mathbf{Ec}
  \label{eq:fockOperator3}
\end{equation}

\bigskip

\noindent Due to the Fock operator containing the molecular orbitals, which themselves depend on the chosen coefficients, the Roothan-Hall equation is solved via an iterative prodecure, where an initial set of coefficients is chosen before the first iteration.
Then, the Fock matrix is constructed and diagonalized, yielding a new set of coefficients as well as energy eigenvalues.
The latter then serve as criteria for convergence, with iterations being carried out until the difference in energy is below a predefined threshold.
The so received approximated wavefunction may then be used for further analysis of the system, e.g.~calculating forces or charges acting upon atoms.

\subsubsection{Post Hartree-Fock Methods}
Hartree-Fock, mainly due to employing only a single Slater determinant, neglects part of electron correlation, i.e.~how electrons in movement are affected by the presence of other electrons.
While some correlation is included in the exchange terms of the Fock operator, neglecting other forms of correlation causes the method to be limited to a theoretical \textit{Hartree-Fock limit}, the lowest energy achievable.
Over the years, many methods were developed to account for said correlation, congregating under the term \textit{Post Hartree-Fock methods}.
One such method is \textit{Configuration Interaction} (CI), where the HF wave function is expanded to include excited determinants, which are then combined linearly to approximate a correlated wave function. Just like in base HF, the vartiational principle is applied when weighing the contribution of excited states.
Including all possible excited states, referred to as \textit{Full Configuration Interaction} (Full-CI), while providing the most accurate results, is computationally unfeasible for most molecules, which is why truncation is usually applied.

\subsection{Density Functional Theory}
Spawning from the field of solid-state physics, \textit{Density Functional Theory} (DFT) is a quantum mechanical method utilizing an alternative approach to many-electron systems with the aim of addressing the inaccuracies of HF and the high computational cost of post-HF methods.
Where wave-functions depend on 3N spatial coordinates, excluding spin, DFT instead focuses on the electron density $\rho(\vec{r})$ of a system, a property defined by only 3 spatial coordinates, theoretically decreasing computational demand drastically~\cite{Orio2009}.
The formalisms and qualitative discussions in the following subsections~\ref{hohenbergKohn} trough~\ref{hybrid} are based on \textit{A Chemists's Guide to Density Functional Theory} by Koch and Holthausen~\cite{Koch2001-yq}.

\subsubsection{Hohenberg-Kohn Theorems}\label{hohenbergKohn}
At the heart of DFT lie two theorems, formulated by Pierre Hohenberg and Walter Kohn in 1964~\cite{Hohenberg1964}.
First, the ground-state of a many electron system, and by extension its properties, are uniquely determined by its electron density $\rho(\vec{r})$. In short, the external potential of a system is a unique functional of the electron density.
Secondly, such a functional, if applied to the ground-state density, will yield the lowest possible energy of the system and this ground-state density corresponds to the exact solution of the Schrödinger equation. 
This, in essence, is the variational principle applied to the electron density functionals, as shown in equation~\ref{eq:hohenbergKohn}.

\begin{equation}
  E[\rho_{Trial}(\vec{r})] \geq E[\rho_{0}(\vec{r})]
  \label{eq:hohenbergKohn}
\end{equation}



\subsubsection{Kohn-Sham Equations}\label{kohnSham}

The \textit{Kohn-Sham} approach (KS), as derived by Kohn and Sham in 1965~\cite{Kohn1965}, forms the basis of current DFT approaches. 
In this approach, a system of non-interacting electrons yielding the electron density of the original problem is constructed as seen in equation~\ref{eq:kohnSham}.

\begin{equation}
  \rho_0(\vec{r}) = \sum_{i}^{N}|\phi_i(\vec{r})|^2
  \label{eq:kohnSham}
\end{equation}

\bigskip

\noindent Ironically, the wavefunction of this non-interacting system is represented by a Slater determinant, referred to as a Kohn-Sham determinant in the context of DFT.
The total energy of the system is then expressed as shown in equation~\ref{eq:kohnSham2}.

\begin{equation}
  E[\rho(\vec{r})] = T_s[\rho(\vec{r})] + J[\rho(\vec{r})] + E_{XC}[\rho(\vec{r})]
  \label{eq:kohnSham2}
\end{equation}

\bigskip

\noindent With $T_s$ being the non-interacting kinetic energy of the system, $J$ the Coulomb energy and $E_{XC}$ the exchange-correlation energy.
While the kinetic energy is for the most part comparatively easy to solve, the exchange-correlation functionals, which also include a "true" part of the kinetic energy, for such a system are unknown, leading to many DFT approaches with the sole purpose of defining increasingly accurate functionals.
Should such an explicit and exact form of the exchange-correlation functional $E_{XC}[\rho(\vec{r})]$ and its corresponding potential $V_{XC}[\rho(\vec{r})]$ be found, the Kohn-Sham equations would, in principal, be exact solutions to many body problems.

\subsubsection{Local Density Approximation}
Among the simplest methods in DFT, \textit{Local (Spin) Density Approximation (L(S)DA)} employs the electron density as a homogenous electron gas.
While effective in describing the solid metals and alloys it was designed for, LDA and its open-shell variant LSDA are considered inadequate for systems involving variable electron densities like MOFs~\cite{Perdew1981}.
In practice, to simplify calculations, the exchange and correlation functionals applied onto the electron density, as shown in equation~\ref{eq:lda}, are considered seperable.

\begin{equation}
  E_{XC}^{LDA}[\rho(\vec{r})] = \int_{-\infty}^{\infty} \epsilon_{XC}[\rho(\vec{r})]\rho(\vec{r})d\vec{r} = \int_{-\infty}^{\infty} \epsilon_{X}[\rho(\vec{r})]\rho(\vec{r})d\vec{r} + \int_{-\infty}^{\infty} \epsilon_{C}[\rho(\vec{r})]\rho(\vec{r})d\vec{r}
  \label{eq:lda}
\end{equation}

\bigskip

\noindent Where $\epsilon_{XC}[\rho(\vec{r})]$ is the exchange-correlation energy per particle and $\rho(\vec{r})$ the electron density, with square brackets denoting a functional dependence.
Since the exchange-energy density of a homogenous electron gas is known analytically, derived by Bloch and Dirach as early as 1929~\cite{Bloch1929}\cite{Dirac1930}, LDA approaches apply this result pointwise to a non-homogenous density to calculate exchange-energy as in equation~\ref{eq:lda2}~\cite{parr1994density}.

\begin{equation}
  E_X^{LDA}[\rho(\vec{r})] = -\frac{3}{4}\left(\frac{3}{\pi}\right)^{1/3}\int_{-\infty}^{\infty}\rho(\vec{r})^{4/3}d\vec{r}
  \label{eq:lda2}
\end{equation}

\bigskip

\noindent While exchange-energy may be explicit, the correlation-energy of a homogeneous electron gas is not, leading to various expressions being derived over time, with e.g~\textit{Monte Carlo} approaches providing accurate results~\cite{Ceperley1980}.

\subsubsection{Generalized Gradient Approximation}
The \textit{Generalized Gradient Approximation} (GGA) differs from LDA by employing not only the electron density $\rho(\vec{r})$, but also its gradient $\nabla\rho(\vec{r})$ with respect to spatial coordinates as in equation~\ref{eq:gga2}.
Functionals utilizing both the electron density and its gradient once more assume a seperation of the exchange and correlation functionals, as shown in equation~\ref{eq:gga1}.

\begin{equation}
  E_{XC}^{GGA}[\rho(\vec{r}), \nabla\rho(\vec{r})] = \int_{-\infty}^{\infty} \epsilon_{XC}[\rho(\vec{r}), \nabla\rho(\vec{r})]\rho(\vec{r})d\vec{r} \\
\label{eq:gga2}
\end{equation}


\begin{equation}
  E_{XC}^{GGA} = E_X^{GGA} + E_C^{GGA}
  \label{eq:gga1}
\end{equation}

\bigskip

\noindent The gradient corrected exchange functional of this approach may be written as in equation~\ref{eq:gga3}.

\begin{equation}
  E_X^{GGA} = E_X^{LDA} - \int_{-\infty}^{\infty} F(s_{\sigma}) \rho_{\sigma}^{4/3}(\vec{r})d\vec{r}
  \label{eq:gga3}
\end{equation}

\bigskip

\noindent Where the argument for the function F is the reduced gradient $s_{\sigma}$ for spin $\sigma$, understood as a local inhomogenity parameter and calculated as in equation~\ref{eq:inhomogenity}.
The value of $s_{\sigma}$ is large for regions of small densities such as those far from the nuclei and small for bonding regions or regions of large density.
For the function F, two main realizations are employed. The first one, based on a GGA functional developed by Becke in 1988~\cite{Becke1988}, approaches inhomogenity as in equation~\ref{eq:gga4}.

\begin{equation}
  s_{\sigma} = \frac{|\nabla\rho_{\sigma}(\vec{r})|}{\rho_{\sigma}^{4/3}(\vec{r})}
  \label{eq:inhomogenity}
\end{equation}

\begin{equation}
  F^{B} = \frac{\beta s^2_{\sigma}}{1 + 6\beta s_{\sigma} \sinh^{-1}s_{\sigma}}
  \label{eq:gga4}
\end{equation}

\bigskip

\noindent In this functional, abbreviated simply as B or B88, the parameter $\beta$ has been empirically determined to a value of 0.0042 by fitting to known exchange energies.
The second class of functionals laying much of the groundwork for modern approaches employ F as a rational function of $s_{\sigma}$, as examplified \textit{via} Perdews's exchange functional P86 in equation~\ref{eq:gga5}~\cite{Perdew1986}.

\begin{equation}
  F^{P86} = \Biggl(1+1.296\biggl(\frac{s_{\sigma}}{(24\pi^2)^{1/3}}\biggl)^2 + 14\biggl(\frac{s_{\sigma}}{(24\pi^2)^{1/3}}\biggl)^4 + 0.2\biggl(\frac{s_{\sigma}}{(24\pi^2)^{1/3}}\biggl)^6 \Biggl)^{1/15}
  \label{eq:gga5}
\end{equation}

\bigskip

\noindent Here, as opposed to the Becke functional, no empirical parameters are present.
With exchange functionals established, gradient-corrected correlation functionals are a different beast entirely when it comes to analytical complexity.
One widely used functional is the P or P86 correlation functional, a counterpart to the 1986 Perdew exchange functional discussed previously, employing an empirical parameter fitted to the correlation energy of a neon atom~\cite{Perdew1986_2}.
Functionals free of empirical parameters have also been developed, such as \textit{PW91} by Perdew and Wang in 1991~\cite{Burke1998}.
Finally and most notably, Lee, Yang and Parr formulated their \textit{LYP} functional in 1988, derived from the \textit{Colle-Salvetti} correlation energy expression~\cite{Lee1988}, in which the correlation energy density is expressed in terms of the electron density as well as a Laplacian of the second-order HF density matrix~\cite{Colle1975}.
While freely combining different exchange and correlation functionals is generally possible, only a small subset of pairings is seeing much use in modern theoretical chemistry, with the combination of Becke's exchange functional and LYP correlation functional, known as BLYP, being one of the most widely used frameworks.


\subsubsection{Hybrid Functionals}\label{hybrid}

In an attempt to further improve the accuracy of DFT calculations, \textit{Hybrid Functionals} utilize a combination of the exact Hartree-Fock exchange energies and approximated correlation functionals, as shown in equation~\ref{eq:hybrid}.

\begin{equation}
  E_{XC} = E_X^{HF} + E_C^{KS}
  \label{eq:hybrid}
\end{equation}

\bigskip

\noindent However, attempting to exploit the strengths of both HF and DFT using reckless combination provided no benefit over previously established approaches, with inaccuracies ranging in the high two-digit $kcal/mol$ range especially when applie to molecules and chemical bonding.
The first succesful hybrid functional was derived by Becke in 1993~\cite{Becke1993_hybrid}, employing a combination of HF exchange and DFT correlation in the `\textit{half-and-half}' approach with already promisingly low average errors.
In the next iteration, ultimately leading to arguably the most widely used hybrid functional, Becke introduced semiempirical coefficients to weigh the various hybrid components~\cite{Becke1993a}. 
This combination of HF exchange with local and gradient-corrected correlation and exchange terms is shown in equation~\ref{eq:becke1}, with the semiempirical parameters $a=0.20$, $b=0.72$ and $c=0.81$ fitted to optimally reproduce previous experimental results. 

\begin{equation}
  E_{XC}^{B3} = E_{XC}^{LSD}+a(E_{XC}^{\lambda = 0}-E_{X}^{LSD})+bE_{X}^{B}+cE_{C}^{PW91}
  \label{eq:becke1}
\end{equation}

\bigskip

\noindent Where $\lambda=0$ denotes an interaction free system and $\lambda=1$ a fully interacting system.
Suggested by Stephens et al.~in 1994, replacing the PW91 correlation functional in equation~\ref{eq:becke1} with a LYP functional, the B3LYP exchange-correlation functional was born~\cite{stephens_ab_1994}, as seen in equation~\ref{eq:b3lyp}.

\begin{equation}
  E_{XC}^{B3LYP} = (1-a) E_{X}^{LSD}+aE_{XC}^{\lambda = 0}+bE_{X}^{B88}+cE_{C}^{LYP}+(1-c)E_C^{LSD}
  \label{eq:b3lyp}
\end{equation}

\bigskip

\noindent Where the values for the parameters a, b and c are directly taken from Becke's original functional.
The B3LYP functional showed promising results, with unsigned errors of only slightly above 2 $kcal/mol$ in respect to a Gaussian-2 reference set, comprised of corrected MP2 calculations~\cite{Curtiss1991}.
Although B3LYP remained a pillar of density functional theory, hybrid functionals saw many more iterations and improvements over the years, with notable examples being the PBE0 model by Adamo and Barone in 1999~\cite{Adamo1999} or the M06 family of functionals by Truhlar and Zhao in 2007~\cite{Zhao2007}.

\subsubsection{Dispersion Corrections}
The following discussions are sourced from a review by Klimeš and Michaelides, published in 2012~\cite{Klime2012}.
\\ \par \noindent One of the main shortcomings of DFT is inherent to the approximations made for XC functionals, namely the inability of most standard functionals to properly describe long-range electron correlation, colloquially referred to as \textit{van der Waals} forces.
Dispersion, in essence the attractive interaction between instantaneous dipoles, is well described as decaying with the sixth power of the distance between two particles.
However, most standard exchange-correlation functionals do not accurately describe this dispersion because instantaneous fluctuations in density are not considered and because only local properties are considered for calculating most XC energies.
These functionals only model binding or repulsion when there is an overlap of the electron densities between two atoms, and since this overlap decreases exponentially with distance, the $-1/r^6$ decay is not accurately modelled.
\\ \par \noindent To counteract this deficit, a number of dispersion correction schemes with varying levels of complexity have been developed, two of which will be discussed in detail.
Among the simplest schemes are density functionals that are specifically fitted in such a way as to reproduce weak interaction around minima, like the \textit{Minnesota functionals} by Zhao and Truhlar~\cite{Zhao2007-oa}.
While still suffering from incorrect asymptotic behaviour, owing to the reference data used for fitting, these functionals can also represent other properties than weak interaction like e.g.~reaction barriers, making them at least somewhat accurate for a wide range of generic problems.
At around the same level of complexity, one may include pseudopotential projection to model dispersion.
While laborious to fit for each element, methods such as the \textit{dispersion corrected atom-centered potentials} (DCACP)~\cite{vonLilienfeld2004} or \textit{local atomic potentials} (LAP)~\cite{Sun2008} have shown promising results.
\\ \par \noindent At the next level of complexity, an additional energy term is added to the DFT energy, accounting for missing long range interaction as in equation~\ref{eq:disp1}, with the dispersion interaction calculated as in equation~\ref{eq:disp2}.

\begin{equation}
  E_{Tot} = E_{DFT} + E_{disp}
  \label{eq:disp1}
\end{equation}

\begin{equation}
  E_{disp} = - \sum_{A,B}\frac{C_{6}^{AB}}{r^6_{AB}}
  \label{eq:disp2}
\end{equation}

\bigskip

\noindent Where $C_{6}^{AB}$ is the dispersion coefficient between atoms A and B and $r_{AB}$ the distance between them.
Generally referred to as \textit{DFT-D}, these methods assume dispersion to be pairwise additive over all elemental pairs A and B.
While computationally inexpensive and therefore widely used, DFT-D methods are limited by their disregard for many-body dispersion as well as the influence of chemical environments and different states, since the coefficents $C_{6}^{AB}$ are kept constant.
Additionally, calculating these constant coefficients often requires experimental input, limiting the number of treatable elements.
Presented by Grimme in 2006, the \textit{DFT-D2} schemes employ a formula coupling ionization potentials with static polarizabilities to calculate dispersion coefficients for many common elements~\cite{Grimme2006}.
Despite their shortcomings, especially for alkali and alkali earth atoms, DFT-D2 methods are among the most widely used dispersion corrections in modern DFT usage.
\\ \par \noindent Generally, DFT-D correction schemes diverge at short distances and must therefore be `damped', as in equation~\ref{eq:disp3}.

\begin{equation}
  E_{disp} = - \sum_{A,B}\frac{C_{6}^{AB}}{r^6_{AB}}f(r_{AB},A,B)
  \label{eq:disp3}
\end{equation}

\bigskip

\noindent Where $f(r_{AB},A,B)$ is a damping function equaling to one for large distances and decreasing the dispersive energy to zero or a constant for small distances.
To avoid causing additional divergence, these damping functions must be carefully adjusted to the chosen XC functional.
\\ \par \noindent Moving further up the `stairway' of complexity, one may employ so called \textit{non-local} correlation functionals, which add non-local correlations to local correlation functionals and do not rely on external experimental data, with e.g.~the \textit{non-local van der Waals density functional} (vdW-DF)~promising high accuracies~\cite{Klime2009_vdW-DF}.
Going beyond even these methods, the assumption of pairwise additivity must be left behind and many-body dispersion considered, though these methods would be too complex to elaborate on in the scope of this thesis.

\subsubsection{Density Functional Tight Binding}
The following derivations are functionally based on an article by Koskinen and Mäkinen titled \textit{Density-functional tight-binding for beginners}, published in 2009~\cite{Koskinen2009}, albeit with stylistic alterations to keep the overall thesis consistent.
The penultimate section regarding DFTB3 is sourced from an article by Gaus \textit{et al.}~titled \textit{DFTB3: Extension of the Self-Consistent-Charge Density-Functional Tight-Binding Method}, published in 2011~\cite{Gaus2011}.
\\ \par \noindent Despite the many advancements in DFT and every increasing chemical accuracy, faster and computationally less demanding methods are always subject of research with the goal of reaching longer timeframes of simulation or upscale the size of studied systems.
Building on the principles of \textit{tight-binding} theories~\cite{Goringe1997},  \textit{Density Functional Tight Binding} (DFTB) is one such method particularly useful for total energy calculations.
To start, the Kohn-Sham energy already presented in a more compact formalism in equation~\ref{eq:kohnSham2} is expanded upon and written as in equation~\ref{eq:dftb2}.

\begin{equation}
  E[\rho(\vec{r})] = T_s + E_{ext} + E_{H} + E_{XC} + E_{II}
  \label{eq:dftb2}
\end{equation}

\bigskip

\noindent Where $T_s$ is, again, the non-interacting kinetic energy, $E_{ext}$ the external potential energy, $E_{H}$ the Hartree energy, $E_{XC}$ the exchange-correlation energy and $E_{II}$ the ion-ion interaction energy.
The exchange energy may be expressed as in equation~\ref{eq:dftb3}, showing how it contains all `leftover' many-body effects.

\begin{equation}
  E_{XC} = (T-T_s) + (E_{ee} - E_{H})
  \label{eq:dftb3}
\end{equation}

\bigskip

\noindent Where $T$ is the total kinetic energy and $E_{ee}$ the total electron-electron interaction energy.
More explicitly, equation~\ref{eq:dftb2} may be written as in equation~\ref{eq:dftb4}.


\begin{equation}
  E[\rho] = \sum_{a}^{} f_a \langle\phi_a|(-\frac{1}{2}\nabla^2 + \int V_{ext}(\vec{r})\rho(\vec{r})d\vec{r})|\phi_a\rangle + \frac{1}{2}\int\int \frac{\rho(\vec{r})\rho(\pvec{r}')}{|\vec{r}-\pvec{r}'|}d\vec{r}d\pvec{r}' + E_{XC}[\rho] + E_{II}
  \label{eq:dftb4}
\end{equation}

\bigskip

\noindent Where a indexes electronic eigenstates $\phi$ and $f_a \in [0,2]$ is the occupation number of such a single-particle state with energy $\epsilon_a$.
In the next step, one considers a reference density $\rho_0(\vec{r})$ composed of atomic densities that does not minimize equation~\ref{eq:dftb4} but is assumed to neighbour the true minimal density as in equation~\ref{eq:dftb5}.

\begin{equation}
  \rho(\vec{r}) = \rho_0(\vec{r}) + \delta\rho(\vec{r})
  \label{eq:dftb5}
\end{equation}

\bigskip

\noindent Where $\delta\rho(\vec{r})$ is a small fluctuation of the electron density. Expanding the Energy at $\rho_0(\vec{r})$ to the second order in fluctuations then leads to equation~\ref{eq:dftb6}.

\begin{align}\label{eq:dftb6}
    E[\delta\rho] \approx &\sum_{a}^{} f_a \langle\psi_a|-\frac{1}{2}\nabla^2+V_{ext} + V_H[\rho_0] + V_{XC}[\rho_0]|\psi_a\rangle \\
    &+ \frac{1}{2}\int\int \Biggl(\frac{\delta^2E_{XC}[\rho_0]}{\delta\rho(\vec{r})\delta\rho(\pvec{r}')}\Biggr)\delta\rho(\vec{r})\delta\rho(\pvec{r}')d\vec{r}d\pvec{r} \notag \\
    &-\frac{1}{2}\int V_H[\rho_0]\delta\rho(\vec{r})d\vec{r} + E_{XC}[\rho_0] + E_{II} - \int V_{XC}[\rho_0](\vec{r})\rho_0(\vec{r})d\vec{r} \notag
\end{align}

\bigskip

\noindent The first line in the above equation~\ref{eq:dftb6} is the band-structure energy of the system as in equation~\ref{eq:dftb7}.

\begin{equation}
  E_{BS} = \sum_{a}^{} f_a \langle\psi_a|H[\rho_0]|\psi_a\rangle
  \label{eq:dftb7}
\end{equation}

\bigskip

\noindent The second line is energy stemming from charge fluctuations composed of coulombic and exchange-interaction contributions, shortened to $E_{coul}$. 
Lastly, the four terms in the third line are collectively called repulsive energy, referred to as $E_{rep}$, due to the ion-ion repulsion term being present.
Contracting equation~\ref{eq:dftb6} using this formalism leads to equation~\ref{eq:dftb8}.

\begin{equation}
  E[\delta\rho] \approx E_{BS} + E_{coul} + E_{rep}
  \label{eq:dftb8}
\end{equation}

\bigskip

\noindent The tight-binding approach is mainly concerned with valence electrons, as core electrons are contained in the repulsive energy term.
Additionally, tight-binding assumes, as the name implies, tightly bound electrons.
As such, the atomic orbitals $\phi_a$ are approximated as in equation~\ref{eq:dftb9} via a minimal local basis, i.e.~only one radial function per angular momentum state.

\begin{equation}
  \psi_a(\vec{r}) = \sum_{\mu}^{}c_{\mu}^a\phi_{\mu}(\vec{r})
  \label{eq:dftb9}
\end{equation}

\bigskip

\noindent With $\mu$ indexing the pseudo-atomic basis states $\phi$ and $\psi_a$ being an electronic eigenstate.
Applying this expansion to equation~\ref{eq:dftb7} leads to equation~\ref{eq:dftb10}.

\begin{equation}
  E_{BS} = \sum_{a}^{} f_a \sum_{\mu\nu}^{}c_{\mu}^{a'} c_{\nu}^a H_{\mu\nu}^0
  \label{eq:dftb10}
\end{equation}
\begin{equation*}
  H^0_{\mu\nu} = \langle\phi_{\mu}|H^0|\phi_{\nu}\rangle
\end{equation*}

\bigskip

\noindent With $H^0_{\mu\nu}$ being the Hamiltonian matrix elements of the reference system.
The values of these matrix elements are pre-computed and stored, leading to a significant increase in computational efficiency and forming the core of the tight-binding approach.
In addition, one must look at atomic populations utilizing an overlap of orbitals, as in equation~\ref{eq:dftb11}.



\begin{equation}
  q_I = \sum_{a}^{}f_a\sum_{\mu \in I}^{}\sum_{\nu}^{}\frac{1}{2}(c_{\mu}^{a*}c_{\nu}^a + c_{\nu}^{a*}c_{\mu}^a)S_{\mu\nu}
  \label{eq:dftb11}
\end{equation}
\begin{equation*}
  S_{\mu\nu} = \langle\psi_{\mu}|\psi_{\nu}\rangle
\end{equation*}

\bigskip

\noindent Where $q_I$ is the population or charge of atom I, S the overlap matrix, $\mu$ and $\nu$ again indexing the atomic basis states and $a$ denoting electronic eigenstates.
The difference in charge to a neutral atom $\Delta q_I$ is then calculated as in equation~\ref{eq:dftb12}.

\begin{equation}
  \Delta q_I = q_I - q_I^{0}
  \label{eq:dftb12}
\end{equation}

\bigskip

\noindent With $q_I^{0}$ being the number of valence electrons in the neutral atom.
Applying undetermined Lagrane multipliers to constrain the wave function norms, one arrives at the Kohn-Sham equation-equivalent of DFTB, as shown in equation~\ref{eq:dftb13}.

\begin{equation}
  H_{\mu\nu} = H_{\mu\nu}^0 + \frac{1}{2}S_{\mu\nu}\sum_{K}^{}(\gamma_{IK} + \gamma_{JK})\Delta q_K  \quad \mu \in I, \nu \in J
  \label{eq:dftb13}
\end{equation}

\bigskip

\noindent Writing the electrostatic potential on atom I due to charge fluctuations as in equation~\ref{eq:dftb14} and $h_{\mu\nu}^1$ as in equation~\ref{eq:dftb15}, one may contract the above equation~\ref{eq:dftb13} to equation~\ref{eq:dftb16}.

\begin{equation}
  \epsilon_I = \sum_{K}^{}\gamma_{IK}\Delta q_K
  \label{eq:dftb14}
\end{equation}
\begin{equation}
  h_{\mu\nu}^1 = \frac{1}{2}(\epsilon_I + \epsilon_J) \quad \mu \in I, \nu \in J
  \label{eq:dftb15}
\end{equation}
\begin{equation}
  H_{\mu\nu} = H_{\mu\nu}^0 + h_{\mu\nu}^1S_{\mu\nu}
  \label{eq:dftb16}
\end{equation}

\bigskip

\noindent As in the DFT equivalent, equation~\ref{eq:dftb16} is solved iteratively until convergence is reached.
This, while suffering from many of the same convergence issues as DFT, is usually notably faster and more efficient than its parent methods~\cite{Elstner2014-zp}.
\\ \par \noindent Apart from the tight-binding formalism, DFTB also approaches the collected term referred to as repulsive energy as well as the coulombic term defined in equation~\ref{eq:dftb8} via semi-empirical approximations.
Firstly, for the repulsive term containing complicated exchange interactions, a repulsive function depending only on atomic numbers, as in equation~\ref{eq:dftb17}, is fitted to higher-level theoretical calculations \textit{via}, mostly cubic, splining.

\begin{equation}
  E_{rep} = \sum_{I<J}^{}V_{rep}^{IJ}(R_{IJ})
  \label{eq:dftb17}
\end{equation}

\bigskip

\noindent Where $V_{rep}^{IJ}$ is the repulsive potential between atoms I and J at a distance $R_{IJ}$.
\\ \par \noindent A final approximation in the DFTB formalism happens at the coulombic term, where the charge fluctuation is written as in equation~\ref{eq:dftb18}.

\begin{equation}
  E_{coul} = \frac{1}{2}\sum_{IJ}^{}\gamma_{IJ}(R_{IJ})\Delta q_I \Delta q_J
  \label{eq:dftb18}
\end{equation}
\begin{equation*}
  \gamma_{IJ}(R_{IJ}) = \begin{cases}
    U_I & \text{for } I = J \\
    \frac{erf(C_{IJ}R_{IJ})}{R_{IJ}} & \text{for } I \neq J
  \end{cases}
\end{equation*}

\bigskip

\noindent Where $\gamma_{IJ}$ is a Coulomb-like function with dampening applied at small atomic distances $R_{IJ}$.
This function $\gamma$ is equal to the so-called Hubbard parameter $U_I$ when $I=J$, which itself is twice the chemical hardness of an atom.
In short, one estimates the size of the charge distribution from absolute hardness, which in turn is used to estimate Coloumb interactions between the atoms.
In this case, the Hubbard parameter $U_I$ for each element is taken from standard tables to determine charge transfer.
\\ \par \noindent 
With all previously mentioned approximations in place, one finally arrives at the DFTB2 formalism, named so since the Kohn-Sham energy is expanded up to the second order. 
These semi-empirical parametres discussed above are stored in so called \textit{Slater-Koster} files, one for each distinct permutation of atom-atom interaction. In summary, these contain Hamiltonian matrix elements and overlap integrals for all relevant orbital interactions, fitted repulsive potentials as well as Hubbar-parameters for each element.
Mainly due to this parametrisation, DFTB2 displays faster convergence than popular DFT methods as well as high transferability, making it a popular choice for large-scale simulations~\cite{Hofer2023}.
\\ \par \noindent
On a final note, the problem of chemical hardness being a constant irrespective of charge state as well as atoms being restricted to fixed shapes via their initial reference densities in DFTB2 is adressed by including the third order expansion of the Kohn-Sham energy, as seen in equation~\ref{eq:dftb19}.

\begin{equation}
  E^{3rd} \approx \frac{1}{6}\sum_{a}^{}\delta q_a^3\frac{\delta\gamma_{aa}}{\delta q_a}\Biggr|_{q_a^0} + \frac{1}{6}\sum_{a \neq b}^{}\Delta q_a \Delta q_b \Biggl( \Delta q_a \frac{\delta \gamma_{ab}}{\delta q_a}\Biggr|_{q_a^0} + \Delta q_b \frac{\delta \gamma_{ab}}{\delta q_b}\Biggr|_{q_b^0} \Biggr)
    \label{eq:dftb19}
\end{equation}

\bigskip

\noindent Where $\frac{\delta\gamma}{\delta q}$ describes the charge dependancy of the Hubbard parameter $U$ as in equation~\ref{eq:dftb20}.

\begin{equation}
  \frac{\delta\gamma_{ab}}{\delta q_a}\Biggr|_{q_a^0} = \frac{\delta\gamma_{ab}\delta U_a}{\delta U_a\delta q_a}\Biggr|_{q_a^0} \quad with \quad a\neq b
  \label{eq:dftb20}
\end{equation}

\bigskip 

\noindent Including this third order expansion leads to the DFTB3 formalism, shown to allow for more flexible description of interatomic electrostatics and charge transfer, as well as a more accurate description of chemical hardness~\cite{yangsource}. In addition to the parametres above, Slater-Koster files for DFTB3 based calculations also contain these Hubbard derivatives, a very popular variant of which is the 3ob parameter set particularly useful for organic systems~\cite{Gaus2012}.

\subsection{Molecular Dynamics Simulations}
Alongside advancements in the field of digital electronics came the possibilities of simulating chemical 
systems on ever larger scales. Where practical experiments are limited by equipment, time, resources and safety regulations, 
simulations can be conducted cost effectively and with an ever increasing level of precision.
Simulating a chemical system to receive meaningful information starts with choosing the most viable
theoretical framework. \textit{Molecular Dynamics} (MD), utilizing the DFTB method, is one of those frameworks particularly well suited for  the theoretical time-dependant analysis of periodic chemical systems. From the output of such a framework, macroscopic properties and structural information of a given system may be inferred using the principles of statistical mechanics.
In this chapter, the most relevant aspects of MD simulations for the scope of this thesis will be discussed, mostly based on \textit{Computer Simulation of Liquids} by Allen and Tildesley unless cited otherwise, again with stylistic changes for the sake of consistency~\cite{Allen2017}.

\subsubsection{Statistical Mechanics - Ensembles}
Unless calculating a system with no kinetic energy, i.e.~at absolute zero and at a state of least entropy, the various states of a chemical system during a simulation will cover a variety of conformations and, therefore, potential and kinetic energies. The potential energy of such a system is a function of the positions of all individual atoms, leading to what is called a potential energy surface (PES), in essence a 3N-dimensional function where N is the number of atoms in the system. To extract macroscopic properties from such a system, however, one must also consider the velocities each atom has at specific points in time, as the total energy of a closed system is the sum of kinetic and potential energies as in equation~\ref{eq:total}

\begin{equation}
  E_{tot}(\textbf{Q}, \textbf{P}) = K(\textbf{P}) + V(\textbf{Q})
  \label{eq:total}
\end{equation}

\bigskip

\noindent Where $\textbf{Q}$ and $\textbf{P}$ are the position and momenta vectors respectively.
The positions and momenta of all atoms are thought of as coordinates in a multidimensional space with 6N dimensions, the \textit{phase space}. Over the course of a simulation, many different phase space vectors are sampled, leading to what is referred to as an \textit{ensemble} of configurations weighted by their probability of occuring. Such an ensemble may be regarded as a collection of all accessible points $\Theta$ in phase space, with different ensembles defined by which macroscopic parametres (volume, pressure, temperature,  ...) are fixed.
\\ \par \noindent
Given a particular point in phase space, one may simply calculate the microscopic value of some property as a function of the phase space vector. According to the principles of the \textit{Ergodic hypothesis}, a central approximation in statistical analysis, the time average of a process parameter equals the observable equilibrium value of said property on a macroscopic scale as desribed in equation~\ref{eq:ergoden}, so long as sufficiently long sampling times are employed. This assumption must be made as MD simulations, limited by computational resources, cannot sample system sizes even close to real world scales and is key to extracting macroscopic properties. In such an \textit{ergodic system}, an equal amount of time is spent in each possible phase state~\cite{Gupta1990-au}.

\begin{equation}
  A_{obs} = \langle A  \rangle_{time} = \bigl\langle A \bigl(\mathbf{\Theta}(t)\bigr) \bigl\rangle_{time} = \lim_{t_{obs} \to \infty} \frac{1}{t_{obs}}\int_{0}^{t_{obs}}A\bigl(\mathbf{\Theta}(t)\bigr)dt
  \label{eq:ergoden}
\end{equation}

\bigskip

\noindent Where $A_{obs}$ is the observable macroscopic value of a property, $A$ the property itself at a given phase space vector $\mathbf{\Theta}(t)$ at time $t$ and $t_{obs}$ the observation time. As it is impossible to extend the observation time to infinity, equations of motions are usually solved over a large but finite number of equally long timesteps, as in equation~\ref{eq:ergoden2}.

\begin{equation}
  A_{obs} = \langle A  \rangle_{time} = \frac{1}{\tau_{obs}}\sum_{\tau=1}^{\tau_{obs}}A\bigl(\mathbf{\Gamma}(t)\bigr)
  \label{eq:ergoden2}
\end{equation}
\begin{equation*}
  \delta t = \frac{t_{obs}}{\tau_{obs}}
\end{equation*}

\bigskip

\noindent Where $\tau_{obs}$ is the number of timesteps each with length $\delta t$.

\subsubsection{The Canonical Ensemble}
While the usual choice for conducting MD simulations of metal-organic frameworks might be the \textit{istothermic-isobaric} conditions in the NPT ensemble, the employed parametrization of nitrogen interactions in the DFTB framework has been shown to lead to high instability and often times full system collapse of the porous structure under these conditions. For this reason, simulations in this thesis are treated under constant volume conditions, equating a \textit{canonical} (NVT) ensemble which has previously been employed to good effect in similar studies~\cite{Fischereder2023}\cite{Purtscher2023}.
\\ \par \noindent
Such a canonical ensemble is defined by a fixed number of particles, volume and temperature, which may be understood as coupling a system to an external heat bath to allow energy exchange. The partition function $Q_{NVT}$ in this ensemble is the sum of probability densities for all possible microstates $\Gamma$ as in equation~\ref{eq:partition}.


\begin{equation}
  Q_{NVT} = \sum_{\mathbf{\Gamma}}^{}\exp\Bigl(-\frac{H(\mathbf{\Gamma})}{k_BT}\Bigl)
  \label{eq:partition}
\end{equation}

\bigskip

\noindent Where $H(\Gamma)$ is the Hamiltonian of the system in a given microstate $\Gamma$ and $k_B$ the \textit{Helmholtz constant}. Since energy fluctuations are non-zero and the energy is comprised of kinetic and potential components, the partition function can be further factorized into kinetic and potential parts as in equation~\ref{eq:partition2}, with $h$ being the \textit{Planck} constant.

\begin{equation}
  Q_{NVT} = \frac{1}{N!}\frac{1}{h^{3N}}\int \exp\Bigl(-\frac{K(\textbf{\textit{P}})}{k_BT}\Bigl)d\textbf{\textit{p}}\int \exp\Bigl(-\frac{V(\textbf{\textit{Q}})}{k_BT}\Bigl)d\textbf{\textit{q}}
  \label{eq:partition2}
\end{equation}

\bigskip

\noindent The relevant thermodynamic function for this ensemble is the \textit{Helmholtz free energy} A, defined as in equation~\ref{eq:helmholtz}.

\begin{equation}
  A=-k_BT \ln Q_{NVT}
  \label{eq:helmholtz}
\end{equation}

\bigskip

\subsubsection{Equations of Motion}
As molecular dynamics simulation aim to describe the time-dependant behaviour of an N particle system interacting through a potential $V$, equations of motion must be solved. In MD theory, this is done classically as described in this chapter, starting at the pillar of classical mechanics, \textit{Isaac Newton's} laws of motion. Most importantly for MD simulations, the second law of motion states that acceleration, i.e.~the derivative of momentun with respect to time, is proportional and parallel to the net force and inversely proportional to the mass of an object, as in equation~\ref{eq:newton}~\cite{Alrasheed2019}.

\begin{equation}
  \textbf{a} = \frac{\partial\textbf{p}}{\partial t} = \frac{\textbf{F}}{m}
  \label{eq:newton}
\end{equation}

\bigskip

\noindent Classical mechanics are insufficient at describing relativistic or quantum mechanical systems, but are however suited for the masses and velocities of atoms in typical systems.  As such, the mass of particles can be considered constant and, with the fact that Force is defined as the negative potential gradient as in~\ref{eq:newton3}, equation~\ref{eq:newton} can be rewritten as in equation~\ref{eq:newton2}.

\begin{equation}
  \textbf{F} = -\frac{\partial V}{\partial\textbf{q}}
  \label{eq:newton3}
\end{equation}

\begin{equation}
  \textbf{a} = \frac{d^2\textbf{q}}{d t^2} = \frac{\textbf{F}}{m}
  \label{eq:newton2}
\end{equation}

\bigskip

\noindent The combination of~\ref{eq:newton2} and~\ref{eq:newton3} finally leads to equation~\ref{eq:newton4}, revealing the Newtonian EOM as a differential equation of second order.

\begin{equation}
  \mathbf{F} = m\frac{d^2\mathbf{q}}{d t^2} = -\frac{\partial V}{\partial\textbf{q}}
  \label{eq:newton4}
\end{equation}

\bigskip

\noindent Solving such a differential equation for N particles to obtain their position vectors $\textbf{q}$ at time $t$ is, even numerically, completely unfeasible for most system sizes.
\\ \par \noindent A solution to this problem starts at a different framework, namely \textit{Lagrangian} equation sof motion as in~\ref{eq:lagrange1}. Here, the Lagrangian function $L(\textbf{q}, \dot{\textbf{q}})$, dependant on spatial coordinates and their time derivatives, is defined as seen in equation~\ref{eq:lagrange2} as the difference between kinetic and potential energy.

\begin{equation}
  \frac{d}{dt}\Biggl(\frac{\partial L}{\partial \dot{\textbf{q}}_k}\Biggr) - \frac{\partial L}{\partial \textbf{q}_k} = 0
  \label{eq:lagrange1}
\end{equation}

\begin{equation}
  L = K - V
  \label{eq:lagrange2}
\end{equation}

\bigskip

\noindent Where $\textbf{q}_k$ and $\dot{\textbf{q}}_k$ are the k-th generalized coordinates and their time derivatives respectively. In combination with the definitions of kinetic energy as in~\ref{eq:kinetic} and potential energies as summations over all individual atoms, pairs, triplets, etc., as in equation~\ref{eq:potential}, the Lagrange equation of motion can be rewritten as in equation~\ref{eq:lagrange3} using Cartesian coordinates $\textbf{r}_i$.

\begin{equation}
  K = \sum_{i = 1}^{N}\sum_{\alpha}^{}p_{i\alpha}^2/2m_i \quad \text{with}\quad \alpha \in x, y, z
  \label{eq:kinetic}
\end{equation}

\begin{equation}
  V = \sum_{i=1}^{N}V_i + \sum_{i = 1}^{N}\sum_{j = i+1}^{N}V_{ij} + \sum_{i = 1}^{N}\sum_{j = i+1}^{N}\sum_{k = j+1}^{N}V_{ijk} + ...
  \label{eq:potential}
\end{equation}

\begin{equation}
  m_i \ddot{\textbf{q}}_i = \textbf{F}_i = -\mathbf{\nabla}_{\textbf{q}_i} V = \mathbf{\nabla}_{\textbf{q}_i} L
  \label{eq:lagrange3}
\end{equation}
\bigskip

\noindent Where $\textbf{F}_i$ is the total force acting on molecule i. The generalized momenta $p_k$ are defined as in equation~\ref{eq:momentum}.

\begin{equation}
  p_k = \frac{\partial L}{\partial \dot{q}_k}
  \label{eq:momentum}
\end{equation}

\bigskip


\noindent Finally, the Hamiltonian equations of motion utilizes the Lagrangian definition of the generalized momentum $p_k$ as in equations~\ref{eq:hamilton1} and~\ref{eq:hamilton2}, with the classical Hamiltonian strictly defined as in equation~\ref{eq:hamilton3}.

\begin{equation}
  H(\mathbf{\Theta}) = H(\textbf{p}, \textbf{q}) = \sum_{k}^{}p_k\dot{q}_k - L(\textbf{q}, \dot{\textbf{q}})
  \label{eq:hamilton3}
\end{equation}

\begin{equation}
  \dot{q}_k = \frac{\partial H}{\partial p_k}
  \label{eq:hamilton1}
\end{equation}

\begin{equation}
  \dot{p}_k = -\frac{\partial H}{\partial q_k}
  \label{eq:hamilton2}
\end{equation}



\bigskip

\noindent Switching to phase-space coordinates, with $i$ indexing individual atoms, Hamiltons equations of motion can be written as in equations~\ref{eq:hamilton4} and~\ref{eq:hamilton5}.

\begin{equation}
  \frac{d\textbf{q}_i}{dt} =\frac{\partial H}{\partial \textbf{p}_i} = \frac{\partial E_{kin}}{\partial \textbf{p}_i} =\frac{\textbf{p}_i}{m_i} = \textbf{v}_i
  \label{eq:hamilton4}
\end{equation}

\begin{equation}
  \frac{d\textbf{p}_i}{dt} = \frac{\partial H}{\partial \textbf{q}_i}= - \frac{\partial V}{\partial \textbf{q}_i}= \textbf{F}_i
  \label{eq:hamilton5}
\end{equation}

\bigskip

\noindent In contrast to Newton's equations of motion, Hamilton's equations require solving N less complex first order differential equations for momenta $\mathbf{p}(t)$ and position $\mathbf{q}(t)$ vectors each.

\subsubsection{The Louiville Formalism}

\noindent Provided that Hamiltons's equations of motion are satisfied, the total time derivative of the Hamiltonian is given in equation~\ref{eq:hamilton6} following simple chain rules.

\begin{equation}
  \frac{dH}{dt} = \frac{\partial H}{\partial \textbf{q}} \frac{\partial \textbf{q}}{\partial t} + \frac{\partial H}{\partial \textbf{p}} \frac{\partial \textbf{p}}{\partial t}
  \label{eq:hamilton6}
\end{equation}

\bigskip

\noindent With the total energy $H$ being conserved in an isolated system and equations of motion~\ref{eq:hamilton4} and~\ref{eq:hamilton5} established above, equation~\ref{eq:hamilton6} can be rewritten as in equation~\ref{eq:hamilton7}.

\begin{equation}
  \frac{dH}{dt} = \frac{\partial H}{\partial \textbf{q}} \frac{\partial H}{\partial \textbf{p}} - \frac{\partial H}{\partial \textbf{p}} \frac{\partial H}{\partial \textbf{q}} = 0
  \label{eq:hamilton7}
\end{equation}

\bigskip

\noindent To abbreviate the above expression, the bilinear \textit{Poisson bracket} operator is introduced as in equation~\ref{eq:hamilton8} for a general function $G(\textbf{q}, \textbf{p})$ dependant on phase space vectors $\textbf{q}$ and $\textbf{p}$.

\begin{equation}
  \frac{dG}{dt} = \frac{\partial G}{\partial \textbf{q}} \frac{\partial H}{\partial \textbf{p}} - \frac{\partial G}{\partial \textbf{p}} \frac{\partial H}{\partial \textbf{q}} = \{G, H\}
  \label{eq:hamilton8}
\end{equation}

\bigskip

\noindent Using this notation, equation~\ref{eq:hamilton7} may be simply expressed as $\{H, H\} = 0$. Alternatively, the \textit{Liouville operator} $iL$ may be used for classical time propagation as a shorthand for the Poisson bracket as in equation~\ref{eq:hamilton9}, again for a general property G~\cite{Frenkel1997}.

\begin{equation}
  \frac{dG}{dt} = \{G, H\} = iLG
  \label{eq:hamilton9}
\end{equation}

\bigskip

\noindent With the Liouville operator defined as in equation~\ref{eq:hamilton10}.

\begin{equation}
  \frac{d}{dt} = iL = \frac{\partial \textbf{q}}{\partial t} \frac{\partial}{\partial \textbf{q}} + \frac{\partial \textbf{p}}{\partial t} \frac{\partial}{\partial \textbf{p} } = \frac{\partial H}{\partial t} \frac{\partial}{\partial \textbf{q}} + \frac{\partial H}{\partial t} \frac{\partial}{\partial \textbf{p} } = \{, H\}
  \label{eq:hamilton10}
\end{equation}

\bigskip

\noindent Applying this operator to the phase space vector $\mathbf{\Theta}$ allows time evolution, with the solution to the Liouville equation being the phase space vector at a later time $t$ as in equation~\ref{eq:hamilton11}.

\begin{equation}
  \mathbf{\Theta}(t) = e^{iLt}\mathbf{\Theta}(0)
  \label{eq:hamilton11}
\end{equation}

\bigskip

\noindent Where $\mathbf{\Theta}(0)$ is the phase space vector at time $t=0$. Splitting the Liouville operator into a configurational part and a momentum part leads to  equation~\ref{eq:hamilton12}.


\begin{equation}
  iL = iL_{\textbf{q}} + iL_{\textbf{p}}
  \label{eq:hamilton12}
\end{equation}

\bigskip

\noindent It is important to note that the Liouville operators do not commute and the exponentials may not be simply split, since order of application is not arbitrary, as shown in equation~\ref{eq:hamilton13}.

\begin{equation}
  e^{iLt} = e^{iL_{\textbf{q}}t + iL_{\textbf{p}}t} \neq e^{iL_{\textbf{p}}t}e^{iL_{\textbf{q}}t}
  \label{eq:hamilton13}
\end{equation}

\bigskip

\noindent Because the individual operators $exp(iL_{\textbf{q}}t)$ and $exp(iL_{\textbf{p}}t)$ can, in many instances, be evaluated exactly on a phase space vector $\mathbf{\Theta}$, it would be useful to express the time evolution \textit{via} both these factors. A solution to this problem is presented in the following chapter.

\subsubsection{The Trotter Theorem}
The derivations in this chapter are based on \textit{Statistical Mechanics: Theory and Molecular Simulation} by Tuckerman~\cite{Tuckerman2010}.
\\ \par \noindent Formulated by Hale Trotter in 1959~\cite{Trotter1959}, the \textit{Trotter theorem} states that for two non commuting operators $A$ and $B$, the following equation~\ref{eq:trotter1} holds true.

\begin{equation}
  e^{A+B} = \lim_{N \to \infty} \Bigl[e^{B/2N}e^{A/N}e^{B/2N}\Bigl]^P
  \label{eq:trotter1}
\end{equation}

\bigskip

\noindent Where $N$ is an integer. Applying this general theorem to the classical operator established before, and by defining a time step $\Delta t = t/N$, one arrives at equation~\ref{eq:trotter2}.

\begin{equation}
  e^{iLt} = \lim_{N \to \infty , \Delta t \to 0} \Bigl[e^{iL_{\textbf{p}}t/2N}e^{iL_{\textbf{q}}t/N}e^{iL_{\textbf{p}}t/2N}\Bigl]^N
  \label{eq:trotter2}
\end{equation}

\bigskip

\noindent This equation states that a classical system may be propagated in time \textit{via} the separate factors $\exp(iL_{\textbf{q}}t/N)$ and $\exp(iL_{\textbf{p}}t/N)$ exactly for a time t, granted that the number of steps taken $P$ are infinite and the timesteps $\Delta t$ are infinitesimally small. For obvious reasons, these limits are unreachable in practice, hence an approximation to $\exp(iLt)$ is made as shown in equation~\ref{eq:trotter3}.

\begin{equation}
  e^{iLt} \approx \Bigl[e^{iL_{\textbf{p}}\Delta t/2}e^{iL_{\textbf{q}}\Delta t}e^{iL_{\textbf{p}}\Delta t/2}\Bigl]^N
  \label{eq:trotter3}
\end{equation}

\bigskip

\noindent An approximate time propagation may then be achieved by performing $N$ steps of length $\Delta t$ using the factorized opator seen in equation~\ref{eq:trotter4}.

\begin{equation}
  e^{iL\Delta t} \approx e^{iL_{\textbf{p}}\Delta t/2}e^{iL_{\textbf{q}}\Delta t}e^{iL_{\textbf{p}}\Delta t/2}
  \label{eq:trotter4}
\end{equation}

\bigskip

\noindent At this point it may be interesting to note that while the error of an individual step $n$ is proportional to $\Delta t^3$, the global error of a trajectory only scales with $\Delta t^2$~\cite{Allen2017}. Applying Trotter splitting now to the phase space vector and inserting into equation~\ref{eq:hamilton11} yields the time evolution of the phase space vector $\mathbf{\Theta}$ as in equation~\ref{eq:trotter5}.

\begin{equation}
  \mathbf{\Theta}(t+\Delta t) = e^{iL\Delta t}\mathbf{\Theta}(t) \approx e^{iL_{\textbf{p}}\Delta t/2}e^{iL_{\textbf{q}}\Delta t}e^{iL_{\textbf{p}}\Delta t/2}\mathbf{\Theta}(t)
  \label{eq:trotter5}
\end{equation}

\bigskip

\subsubsection{The Velocity Verlet Algorithm}
Derived from the Trotter approach, a widely used numerical time integrator has been derived, the \textit{Verlet} algorithm~\cite{Verlet1967}. A more commonly used and demonstrative iteration of this algorithm has been developed by Swope et al.~\cite{Swope1982}, with the main deviation being that velocities and positions are calculated at the same value of time in this approach. This algorithm, starting from initial atomic positions, velocities and forces, acts over a timestep from $t$ to $t + \Delta t$ as in equations~\ref{eq:verlet1},~\ref{eq:verlet2} and~\ref{eq:verlet3}.

\begin{gather}
  \textbf{v}\Bigl(t + \frac{\Delta t}{2}\Bigr) = \textbf{v}(t) + \frac{\Delta t}{2}\ \textbf{a}(t) =\textbf{v}(t) + \frac{\Delta t}{2}\frac{\textbf{F}_t}{m}
  \label{eq:verlet1}\\[5pt]
  \textbf{q}\Bigl(t + \Delta t\Bigr) = \textbf{q}(t) + \Delta t\ \textbf{v}\Bigl(t + \frac{\Delta t}{2}\Bigr)
  \label{eq:verlet2}\\[5pt]
  \textbf{v}\Bigl(t + \Delta t\Bigr) = \textbf{v}\Bigl(t + \frac{\Delta t}{2}\Bigr) + \frac{\Delta t}{2}\ \textbf{a}\Bigl(t + \Delta t\Bigr) = \textbf{v}\Bigl(t + \frac{\Delta t}{2}\Bigr) + \frac{\Delta t}{2}\frac{\textbf{F}_{t + \Delta t}}{m}
  \label{eq:verlet3}
\end{gather}

\bigskip

\noindent At first, equation~\ref{eq:verlet1} may be thought of as `half-advancing' the initial velocity to a midpoint time $t + \Delta t/2$ using accelerations at time $t$. These midpoint velocities are then used as in equation~\ref{eq:verlet2} to propel the coordinates to time $t + \Delta t$. Finally, the endpoint velocities at time $t + \Delta t$ are calculated as in equation~\ref{eq:verlet3} using the endpoint forces as retrieved \textit{via} equation~\ref{eq:verlet4} from the respective endpoint positions.

\begin{equation}
  \textbf{F}_{t + \Delta t} = -\frac{\partial{V}}{\partial \textbf{q}_{t + \Delta t}}
  \label{eq:verlet4}
\end{equation}

\bigskip

\noindent Easily translatable into high-performance code, this algorithm allows for extracting macroscopic properties like e.g. total kinetic energy by simply summing up the squared velocities at a given time, whereas the potential energy is calculated in each step anyway and can simply be stored as output.

\subsubsection{Constraint Dynamics}
As mentioned before, the error over the total trajectory scales with $\Delta t^2$, leading to the setup of a molecular dynamics simulation becoming a balance act of choosing a small enough timestep to accurately represent even the fastest movements of a system with low error, while still being large enough to keep total simulation times in a reasonable frame. Usually, the fastest movement in any given system are X-H bond vibrations, typically oscillating at a period of around 10 fs~\cite{Wang2019}. Representing such movement then dictates going well beyond this timeframe up to an order of magnitude, e.g. 1 fs, vastly increasing computational cost~\cite{Braun2019}.
\\ \par \noindent Luckily, should X-H bond vibrations be of no special concern, constraining certain bonds to a pre-determined length is a viable and commonly used way of increasing timestep size without losing out on vital information. The original method of constraint schemes designed to work in tandem with the verlet operator is the SHAKE algorithm by Ryckaert \textit{et al.}~\cite{Ryckaert1977}. Further developed for the velocity verlet variant by Andersen, the RATTLE algorithm~\cite{Andersen1983} utilizes Lagrange multipliers as demonstrated on a water molecule with fixed OH bonds in the following~\cite{Allen2017}. Oxygen is indexed with 2, whereas the two Hydrogen atoms are labelled 1 and 3 respectively. The constraint equations for the OH bonds are as in equations~\ref{eq:constraint1} and~\ref{eq:constraint2}.

\begin{align}
  \chi^{(q)}_{12}(t) = |\mathbf{q}_{12}(t)|^2 - d^2_{12} = 0, \quad \chi^{(q)}_{23}(t) = |\mathbf{q}_{23}(t)|^2 - d^2_{23} = 0
  \label{eq:constraint1} \\
  \chi^{(p)}_{12}(t) = \mathbf{q}_{12}(t)\cdot\mathbf{p}_{12}(t) = 0, \quad \chi^{(p)}_{23}(t) = \mathbf{q}_{23}(t)\cdot\mathbf{p}_{23}(t) = 0
  \label{eq:constraint2}
\end{align}

\bigskip

\noindent Where $\mathbf{q}_{ij}$ and $\mathbf{p}_{ij}$ are the relative position and momenta vectors of atoms $i$ and $j$ respectively, and $d_{ij}$ is the desired bond length. Equations of motion for constrained atoms now take the form of equation~\ref{eq:constraint3}.

\begin{equation}
  m_i\mathbf{a}_i = \textbf{F}_i + \mathbf{G}_i
  \label{eq:constraint3}
\end{equation}

\bigskip

\noindent Where $\mathbf{G}_i$ is the constraint force acting on atom $i$ ensuring that the above constraint equations are satisfied at all times, defined as in equation~\ref{eq:constraint4}.

\begin{equation}
  \mathbf{G}_i = \Lambda_{12} \mathbf{\nabla}_{\mathbf{q}_{12}}\chi^{(q)}_{12} + \Lambda_{23} \nabla_{\mathbf{q}_{23}}\chi^{(q)}_{23}
  \label{eq:constraint4}
\end{equation}

\bigskip

\noindent Where $\Lambda_{ij}$ are the undetermined Lagrange multipliers. Suggested by Ryckaert \textit{et al.}, the constraint forces are calculated so as to guarantee them being satisfied at each timestep, leading to the same error margins as the integrator they are applied to. Considering the first half-step of the velocity Verlet algorithm, equations~\ref{eq:constraint5} and~\ref{eq:constraint6} show the non-constrained absolute positions and velocities at time $t + \Delta t/2$.


\begin{gather}
  \textbf{v}'_i\Bigl(t + \frac{\Delta t}{2}\Bigr) =\textbf{v}_i(t) + \frac{\Delta t}{2m_i}\textbf{F}_i(t)
  \label{eq:constraint5}\\[5pt]
  \textbf{q}'_i(t + \Delta t) = \textbf{q}(t) + \Delta t\ \textbf{v}'_i\Bigl(t + \frac{\Delta t}{2}\Bigr) = \textbf{q}(t) + \Delta t \ \textbf{v}_i(t) + \frac{\Delta t^2}{2m_i}\textbf{F}_i(t)
  \label{eq:constraint6}
\end{gather}

\bigskip

\noindent Where the prime denotes non-constrained values. Including constraint forces concerning positions then leads to equation~\ref{eq:constraint7}.

\begin{equation}
  \mathbf{q}_i(t + \Delta t) = \mathbf{q}'_i(t + \Delta t) + \frac{\Delta t^2}{2m_i} \textbf{G}_i^{\mathbf{(q)}}(t)
  \label{eq:constraint7}
\end{equation}

\bigskip

\noindent The constraint forces $\textbf{G}_i^{\mathbf{(q)}}$ acting on the three atoms must act along the bond vectors $\textbf{q}_{12}$ and $\textbf{q}_{23}$, as well as adhere to Newton's third law, as in equations~\ref{eq:constraint8},~\ref{eq:constraint9} and ~\ref{eq:constraint10}.


\begin{align}
    &\Bigl(\frac{1}{2}\Delta t^2\Bigr)\mathbf{G}_1^{\mathbf{(q)}}(t) = \lambda_{12}\mathbf{q}_{12}(t)
    \label{eq:constraint8}\\[5pt]
    &\Bigl(\frac{1}{2}\Delta t^2\Bigr)\mathbf{G}_2^{\mathbf{(q)}}(t) = -\lambda_{12}\mathbf{q}_{12}(t) + \lambda_{23}\mathbf{q}_{23}(t)
    \label{eq:constraint9}\\[5pt]
    &\Bigl(\frac{1}{2}\Delta t^2\Bigr)\mathbf{G}_3^{\mathbf{(q)}}(t) = -\lambda_{23}\mathbf{q}_{23}(t)
    \label{eq:constraint10}
\end{align}


\bigskip

\noindent Where $\lambda_{ij}$ are the undetermined Lagrange multipliers. Inserting into equation~\ref{eq:constraint7}, bond vectors at the point $t + \Delta t$ are then calculated as in equations~\ref{eq:constraint11} and~\ref{eq:constraint12}. $\mathbf{q'}_{ij}$ again denotes the non-constrained bond vectors. 

\begin{align}
  \mathbf{q}_{12}(t + \Delta t) &= \mathbf{q}'_{12}(t + \Delta t) + (m_1^{-1} + m_2^{-1})\lambda^{(\mathbf{q})}_{12}\mathbf{q}_{12}(t)-m_2^{-1}\lambda^{(\mathbf{q})}_{23}\mathbf{q}_{23}(t) 
  \label{eq:constraint11}\\[5pt]
  \mathbf{q}_{23}(t + \Delta t) &= \mathbf{q}'_{23}(t + \Delta t) - m_2^{-1}\lambda^{(\mathbf{q})}_{12}\mathbf{q}_{12}(t)+(m_2^{-1} + m_3^{-1})\lambda^{(\mathbf{q})}_{23}\mathbf{q}_{23}(t)
  \label{eq:constraint12}
\end{align}

\bigskip


\noindent Taking then the square modulus of both sides and applying the constraints of the form $|\mathbf{q}_{ij}(t + \Delta t)|^2 =|\mathbf{q}_{ij}(t)|^2 = d_{ij}^2$ as defined in equation~\ref{eq:constraint1} results in a pair of quadratic equations, iteratively solvable for the two undetermined multipliers $\lambda_{12}$ and $\lambda_{23}$. These multipliers may then be inserted into equations~\ref{eq:constraint8},~\ref{eq:constraint9} and~\ref{eq:constraint10} to receive the constraint forces acting on the positions, which in turn allow absolute atomic positions at time $t + \Delta t$ to be calculated \textit{via} equation~\ref{eq:constraint7}.
\\ \par \noindent In the same manner, unconstrained half-step velocities, equation~\ref{eq:constraint5}, are adjusted according to equation~\ref{eq:constraint13}.

\begin{equation}
  \mathbf{v}_i\Bigl(t + \frac{\Delta t}{2}\Bigr) = \mathbf{v}'_i\Bigl(t + \frac{\Delta t}{2}\Bigr) + \frac{\Delta t}{2m_i}\mathbf{G}_i^{\mathbf{(q)}}(t)
  \label{eq:constraint13}
\end{equation}

\bigskip

\noindent Leading to the constrained full-step velocities as in equation~\ref{eq:constraint14}.

\begin{equation}
  \mathbf{v}_i(t + \Delta t) =\mathbf{v}_i\Bigl(t+\frac{\Delta t}{2}\Bigr) + \frac{\Delta t}{2m_i}\mathbf{F}_i(t + \Delta t)+ \frac{\Delta t}{2m_i}\mathbf{G}_i^{\mathbf{(v)}}(t+\Delta t)
  \label{eq:constraint14}
\end{equation}

\bigskip

\noindent Where $\mathbf{G}_i^{\mathbf{(v)}}$ are the constraint forces acting on the velocities, which again are directed along the bond vectors, leading to a similar set of equations as for coordinates before, shown in equations~\ref{eq:constraint15},~\ref{eq:constraint16} and~\ref{eq:constraint17}.

\begin{align}
  &\Bigl(\frac{1}{2}\Delta t\Bigr)\mathbf{G}_1^{\mathbf{(v)}}(t + \Delta t) = \lambda_{12}^{(\mathbf{v})}\mathbf{q}_{12}(t + \Delta t)
  \label{eq:constraint15}\\[5pt]
  &\Bigl(\frac{1}{2}\Delta t\Bigr)\mathbf{G}_2^{\mathbf{(v)}}(t + \Delta t) = -\lambda_{12}^{(\mathbf{v})}\mathbf{q}_{12}(t + \Delta t) + \lambda_{23}^{(\mathbf{v})}\mathbf{q}_{23}(t + \Delta t)
  \label{eq:constraint16}\\[5pt]
  &\Bigl(\frac{1}{2}\Delta t\Bigr)\mathbf{G}_3^{\mathbf{(v)}}(t + \Delta t) = -\lambda_{23}^{(\mathbf{v})}\mathbf{q}_{23}(t + \Delta t)
  \label{eq:constraint17}
\end{align}

\bigskip

\noindent Inserting these expressions for constraint forces into the equation for full-step constrained velocities, equation~\ref{eq:constraint14}, leads to a new set of equations for the undetermined multipliers $\lambda_{12}^{(\mathbf{v})}$ and $\lambda_{23}^{(\mathbf{v})}$,~\ref{eq:constraint18} and~\ref{eq:constraint19}.

\begin{align}
  \mathbf{v}_{12}(t + \Delta t) &= \mathbf{v}'_{12}(t + \Delta t) + (m_1^{-1} + m_2^{-1})\lambda^{(\mathbf{v})}_{12}\mathbf{q}_{12}(t)-m_2^{-1}\lambda^{(\mathbf{v})}_{23}\mathbf{q}_{23}(t) 
  \label{eq:constraint18}\\[5pt]
  \mathbf{v}_{23}(t + \Delta t) &= \mathbf{v}'_{23}(t + \Delta t) - m_2^{-1}\lambda^{(\mathbf{v})}_{12}\mathbf{q}_{12}(t)+(m_2^{-1} + m_3^{-1})\lambda^{(\mathbf{v})}_{23}\mathbf{q}_{23}(t)
  \label{eq:constraint19}
\end{align}

\bigskip

\noindent With bond vectors $\mathbf{q}_{ij}$ already determined in the steps before. Since the constraint equations~\ref{eq:constraint2} are scalar products, the above equations are linear in the unknowns $\lambda_{12}$ and $\lambda_{23}$, which in turn are used in equations~\ref{eq:constraint15},~\ref{eq:constraint16} and~\ref{eq:constraint17} to receive constraint forces and subsequently equation~\ref{eq:constraint14} for absolute velocities at time $t + \Delta t$.

\subsubsection{Periodic Boundary Conditions}
While simulating small or few molecules is a computationally inexpensive and generally fast endeavor, especially on modern hardware, properties of e.g.~liquids in bulk may not be accurately extrapolated from small-scale systems. Increasing then the number of molecules towards a system more representative of a macroscopic bulk however leads to both unfeasibly high costs and a significant number of molecules being present on the bulks surface, experiencing different potentials and therefore skewing the overall simulation.
\\ \par \noindent To counteract these surface effects, \textit{periodic boundary conditions} may be employed~\cite{Allen2017,Cramer2004-vy,Tuckerman2010}. The system being modeled is hereby assumed to be a cubic unit cell of an ideal and infinite crystal, replicated periodically and infinitely in all three dimensions, thus eliminating surface effects. As a particle moves, so do its ifninite mirror particles, and should it leave the box, one of its images will instantly replace it on the opposite face, keeping the number of particles constant. When employed, distances between particles are calculated \textit{via} \textit{minimum image convention}, stating that in the infinitely repeating array of unit cells, a particle $i$ interacts with another particle $j$ to which it is closest, even if that particle is located in one of the neighboring periodic cells. For any given dimension, this convention may be expressed as in equation~\ref{eq:pbc1}.

\begin{equation}
  \Delta r_{ij,\alpha}' = r_{ij,\alpha} - L_\alpha\cdot \text{round}\Bigl(\frac{r_{ij,\alpha}}{L_\alpha}\Bigr) \quad \text{with} \quad \alpha \in x, y, z
  \label{eq:pbc1}
\end{equation}

\bigskip

\noindent Where $r_{ij,\alpha}$ is the non-imaged distance between particles $i$ and $j$, $L_\alpha$ is the length of the unit cell and $\Delta r_{ij,\alpha}'$ is the imaged distance between particles $i$ and $j$, each in dimension $\alpha$. The round function rounds the fraction to the nearest integer.
\\ \par \noindent A problem arising with periodicity is the need to limit the number of interactions to be calculated, since otherwise one would theoretically have to consider an infinite number of interactions. This is usually done by definining a cutoff radius $r_{cut}$, at most as long as half the length of the unit cell, beyond which interactions are considered negligible. To smoothly taper interactions around the cutoff region, switching functions are commonly employed. \\ \par \noindent As a final trick of the trade to increase efficiency, a list of particles $j$ within the cutoff radius of particle $i$ may be defined, known as a \textit{Verlet neighbor list}. Updated periodically, this list ensures that an algorithm does not have to loop over every pair $ij$ in the system at every step to determine if they are within the cutoff radius, but only over the pairs $ij$ in the list. Computational overhead is thus reduced indirectly proportional to however often the list is updated, at the cost of increased memory usage.

\subsubsection{Temperature Control}
In an NVT ensemble, temperature coupling is employed to scale the temperature of a system to a pre-defined value, representing the coupling of a system to an external heat bath. As the temperature of a given system is the sum of the kinetic energies of all particles, temperature coupling algorithms modify the velocities of particles to achieve the desired target temperature. In its simplest form, velocities are scaled in each timestep by a factor $\lambda$ as in equation~\ref{eq:heat2}, calculated as in equation~\ref{eq:temp1}.

\begin{equation}
  \textbf{v}_i' = \lambda \textbf{v}_i
  \label{eq:heat2}
\end{equation}

\begin{equation}
  \lambda = \sqrt{\frac{T_{Target}}{T_{System}}}
  \label{eq:temp1}
\end{equation}

\bigskip

\noindent Where $v_i$ and $v_i'$ are the velocities before and after scaling, $T_{Target}$ is the desired temperature and $T_{System}$ is the temperature of the system. Such a \textit{strong coupling} algorithm is computationally inexpensive, but leads to highly unrealistic trajectories as any movement is immediately dampened. A more sensible approach is the \textit{weak coupling} algorithm, presented by Berendsen \textit{et al.}~\cite{Berendsen1984}, where the rescaling factor $\lambda$ is balanced with a latency factor $\tau$ representing time-lagged heat exchange to the external heat bath. The rescaling factor $\lambda$ is then calculated as in equation~\ref{eq:temp2}.

\begin{equation}
  \lambda = \sqrt{1 + \frac{\Delta t}{\tau}\Bigl(\frac{T_{Target}}{T_{System}} - 1\Bigr)}
  \label{eq:temp2}
\end{equation}

\bigskip

\noindent Where $\Delta t$ is the timestep and $\tau$ is the coupling time constant, usually in the range of 0.1 to 1 ps. Since this also suppresses kinetic fluctuations that could be considered natural and physically realistic, the Berendsen thermostat does, stricty speaking, not represent a true NVT ensemble. It has however been shown that the approximation is sufficient for large enough systems in the order of hundreds of particles~\cite{Morishita2000}. A number of improvements to thermic coupling have been developed since, with notable examples being the \textit{Bussi-Donadio-Parrinello} thermostat~\cite{Bussi2007} and the \textit{Nosé-Hoover} thermostat~\cite{Hoover1996}, which will both however not be explored in this thesis.

\bibliography{thesis}

\end{document}